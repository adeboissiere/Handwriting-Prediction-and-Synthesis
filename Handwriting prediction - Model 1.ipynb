{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwriting prediction - Model 1\n",
    "\n",
    "This notebook is a personal attempt at coding Alex Graves RNN to predict handwriting (section 4). The paper can be found [here](https://arxiv.org/abs/1308.0850). \n",
    "\n",
    "The goal of this notebook is to implement a network in a straightforward manner. As such, code readability is a priority over performance. The implemented network is consists of layers of LSTM followed by a Gaussian mixtures layer. Handwriting is highly variable. It makes more sense to generate a probabilty density function at each time step for the next stroke to capture that essence.\n",
    "\n",
    "The network appears to be working and generates sequences from a starting point that look like handwriting. It is interesting to note that when generating a sequence, the network chooses a style at random and sticks with it.\n",
    "\n",
    "![example of sample](./pictures/sample1.png)\n",
    "\n",
    "The network is tweakable in number of layers, sequence length, number of mixtures and dropout probability.\n",
    "\n",
    "The notebook is divided into data treatment (I used [hardmaru's code](https://github.com/hardmaru/write-rnn-tensorflow/) for that as that part is boring), network class, loss function and training. \n",
    "\n",
    "The data comes from [IAM On-Line Handwriting Database](http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database). Download data/lineStrokes-all.tar.gz after signing up ! The path should be ./data/lineStrokes if you want to use this notebook.\n",
    "\n",
    "Enjoy :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import svgwrite\n",
    "from IPython.display import SVG, display\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "from ipywidgets import FloatProgress\n",
    "\n",
    "use_cuda = False\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Network configuration\n",
    "n_batch = 20\n",
    "sequence_length = 300\n",
    "hidden_size = 256\n",
    "n_layers = 3\n",
    "n_gaussians = 10\n",
    "gradient_threshold = 10\n",
    "dropout = 0.2\n",
    "\n",
    "# Small number to avoid log(0) issue\n",
    "eps = float(np.finfo(np.float32).eps)\n",
    "\n",
    "# The network could use the extra space :)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "This code comes from [Hardamru](https://raw.githubusercontent.com/hardmaru/write-rnn-tensorflow/master/utils.py). Big thanks to his author !\n",
    "\n",
    "That part is not that fun. Dataloader is a class that parses all the .xml files. It creates a pickle file for future use. It creates a training set and a validation set in batches depending on the sequence length. Function `next_batch()` neatly returns a batch. Use `reset_batch_pointer()` to reset the current batch. See the training function for a proper use of that wonderful code. \n",
    "\n",
    "Some examples of training data :\n",
    "\n",
    "![batch1](./pictures/batch1.png)\n",
    "![batch2](./pictures/batch2.png)\n",
    "\n",
    "And some example code to load the data :\n",
    "\n",
    "```python\n",
    "data_loader = DataLoader(n_batch, sequence_length, 20) # 20 = datascale\n",
    "for i in range(2):\n",
    "    draw_strokes(random.choice(data_loader.raw_data))\n",
    "\n",
    "\n",
    "sample = random.choice(data_loader.raw_data)\n",
    "draw_strokes_random_color(sample, per_stroke_mode = False)\n",
    "draw_strokes_random_color(sample)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounds(data, factor):\n",
    "    min_x = 0\n",
    "    max_x = 0\n",
    "    min_y = 0\n",
    "    max_y = 0\n",
    "\n",
    "    abs_x = 0\n",
    "    abs_y = 0\n",
    "    for i in range(len(data)):\n",
    "        x = float(data[i, 0]) / factor\n",
    "        y = float(data[i, 1]) / factor\n",
    "        abs_x += x\n",
    "        abs_y += y\n",
    "        min_x = min(min_x, abs_x)\n",
    "        min_y = min(min_y, abs_y)\n",
    "        max_x = max(max_x, abs_x)\n",
    "        max_y = max(max_y, abs_y)\n",
    "\n",
    "    return (min_x, max_x, min_y, max_y)\n",
    "\n",
    "# old version, where each path is entire stroke (smaller svg size, but\n",
    "# have to keep same color)\n",
    "\n",
    "\n",
    "def draw_strokes(data, factor=10, svg_filename='sample.svg'):\n",
    "    min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
    "    dims = (50 + max_x - min_x, 50 + max_y - min_y)\n",
    "\n",
    "    dwg = svgwrite.Drawing(svg_filename, size=dims)\n",
    "    dwg.add(dwg.rect(insert=(0, 0), size=dims, fill='white'))\n",
    "\n",
    "    lift_pen = 1\n",
    "\n",
    "    abs_x = 25 - min_x\n",
    "    abs_y = 25 - min_y\n",
    "    p = \"M%s,%s \" % (abs_x, abs_y)\n",
    "\n",
    "    command = \"m\"\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if (lift_pen == 1):\n",
    "            command = \"m\"\n",
    "        elif (command != \"l\"):\n",
    "            command = \"l\"\n",
    "        else:\n",
    "            command = \"\"\n",
    "        x = float(data[i, 0]) / factor\n",
    "        y = float(data[i, 1]) / factor\n",
    "        lift_pen = data[i, 2]\n",
    "        p += command + str(x) + \",\" + str(y) + \" \"\n",
    "\n",
    "    the_color = \"black\"\n",
    "    stroke_width = 1\n",
    "\n",
    "    dwg.add(dwg.path(p).stroke(the_color, stroke_width).fill(\"none\"))\n",
    "\n",
    "    dwg.save()\n",
    "    display(SVG(dwg.tostring()))\n",
    "\n",
    "\n",
    "def draw_strokes_eos_weighted(\n",
    "        stroke,\n",
    "        param,\n",
    "        factor=10,\n",
    "        svg_filename='sample_eos.svg'):\n",
    "    c_data_eos = np.zeros((len(stroke), 3))\n",
    "    for i in range(len(param)):\n",
    "        # make color gray scale, darker = more likely to eos\n",
    "        c_data_eos[i, :] = (1 - param[i][6][0]) * 225\n",
    "    draw_strokes_custom_color(\n",
    "        stroke,\n",
    "        factor=factor,\n",
    "        svg_filename=svg_filename,\n",
    "        color_data=c_data_eos,\n",
    "        stroke_width=3)\n",
    "\n",
    "\n",
    "def draw_strokes_random_color(\n",
    "        stroke,\n",
    "        factor=10,\n",
    "        svg_filename='sample_random_color.svg',\n",
    "        per_stroke_mode=True):\n",
    "    c_data = np.array(np.random.rand(len(stroke), 3) * 240, dtype=np.uint8)\n",
    "    if per_stroke_mode:\n",
    "        switch_color = False\n",
    "        for i in range(len(stroke)):\n",
    "            if switch_color == False and i > 0:\n",
    "                c_data[i] = c_data[i - 1]\n",
    "            if stroke[i, 2] < 1:  # same strike\n",
    "                switch_color = False\n",
    "            else:\n",
    "                switch_color = True\n",
    "    draw_strokes_custom_color(\n",
    "        stroke,\n",
    "        factor=factor,\n",
    "        svg_filename=svg_filename,\n",
    "        color_data=c_data,\n",
    "        stroke_width=2)\n",
    "\n",
    "\n",
    "def draw_strokes_custom_color(\n",
    "        data,\n",
    "        factor=10,\n",
    "        svg_filename='test.svg',\n",
    "        color_data=None,\n",
    "        stroke_width=1):\n",
    "    min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
    "    dims = (50 + max_x - min_x, 50 + max_y - min_y)\n",
    "\n",
    "    dwg = svgwrite.Drawing(svg_filename, size=dims)\n",
    "    dwg.add(dwg.rect(insert=(0, 0), size=dims, fill='white'))\n",
    "\n",
    "    lift_pen = 1\n",
    "    abs_x = 25 - min_x\n",
    "    abs_y = 25 - min_y\n",
    "\n",
    "    for i in range(len(data)):\n",
    "\n",
    "        x = float(data[i, 0]) / factor\n",
    "        y = float(data[i, 1]) / factor\n",
    "\n",
    "        prev_x = abs_x\n",
    "        prev_y = abs_y\n",
    "\n",
    "        abs_x += x\n",
    "        abs_y += y\n",
    "\n",
    "        if (lift_pen == 1):\n",
    "            p = \"M \" + str(abs_x) + \",\" + str(abs_y) + \" \"\n",
    "        else:\n",
    "            p = \"M +\" + str(prev_x) + \",\" + str(prev_y) + \\\n",
    "                \" L \" + str(abs_x) + \",\" + str(abs_y) + \" \"\n",
    "\n",
    "        lift_pen = data[i, 2]\n",
    "\n",
    "        the_color = \"black\"\n",
    "\n",
    "        if (color_data is not None):\n",
    "            the_color = \"rgb(\" + str(int(color_data[i, 0])) + \",\" + str(\n",
    "                int(color_data[i, 1])) + \",\" + str(int(color_data[i, 2])) + \")\"\n",
    "\n",
    "        dwg.add(dwg.path(p).stroke(the_color, stroke_width).fill(the_color))\n",
    "    dwg.save()\n",
    "    display(SVG(dwg.tostring()))\n",
    "\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(\n",
    "            self,\n",
    "            batch_size=50,\n",
    "            seq_length=300,\n",
    "            scale_factor=10,\n",
    "            limit=500):\n",
    "        self.data_dir = \"./data\"\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.scale_factor = scale_factor  # divide data by this factor\n",
    "        self.limit = limit  # removes large noisy gaps in the data\n",
    "\n",
    "        data_file = os.path.join(self.data_dir, \"strokes_training_data.cpkl\")\n",
    "        raw_data_dir = self.data_dir + \"/lineStrokes\"\n",
    "\n",
    "        if not (os.path.exists(data_file)):\n",
    "            print(\"creating training data pkl file from raw source\")\n",
    "            self.preprocess(raw_data_dir, data_file)\n",
    "\n",
    "        self.load_preprocessed(data_file)\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, data_dir, data_file):\n",
    "        # create data file from raw xml files from iam handwriting source.\n",
    "\n",
    "        # build the list of xml files\n",
    "        filelist = []\n",
    "        # Set the directory you want to start from\n",
    "        rootDir = data_dir\n",
    "        for dirName, subdirList, fileList in os.walk(rootDir):\n",
    "            #print('Found directory: %s' % dirName)\n",
    "            for fname in fileList:\n",
    "                #print('\\t%s' % fname)\n",
    "                filelist.append(dirName + \"/\" + fname)\n",
    "\n",
    "        # function to read each individual xml file\n",
    "        def getStrokes(filename):\n",
    "            tree = ET.parse(filename)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            result = []\n",
    "\n",
    "            x_offset = 1e20\n",
    "            y_offset = 1e20\n",
    "            y_height = 0\n",
    "            for i in range(1, 4):\n",
    "                x_offset = min(x_offset, float(root[0][i].attrib['x']))\n",
    "                y_offset = min(y_offset, float(root[0][i].attrib['y']))\n",
    "                y_height = max(y_height, float(root[0][i].attrib['y']))\n",
    "            y_height -= y_offset\n",
    "            x_offset -= 100\n",
    "            y_offset -= 100\n",
    "\n",
    "            for stroke in root[1].findall('Stroke'):\n",
    "                points = []\n",
    "                for point in stroke.findall('Point'):\n",
    "                    points.append(\n",
    "                        [float(point.attrib['x']) - x_offset, float(point.attrib['y']) - y_offset])\n",
    "                result.append(points)\n",
    "\n",
    "            return result\n",
    "\n",
    "        # converts a list of arrays into a 2d numpy int16 array\n",
    "        def convert_stroke_to_array(stroke):\n",
    "\n",
    "            n_point = 0\n",
    "            for i in range(len(stroke)):\n",
    "                n_point += len(stroke[i])\n",
    "            stroke_data = np.zeros((n_point, 3), dtype=np.int16)\n",
    "\n",
    "            prev_x = 0\n",
    "            prev_y = 0\n",
    "            counter = 0\n",
    "\n",
    "            for j in range(len(stroke)):\n",
    "                for k in range(len(stroke[j])):\n",
    "                    stroke_data[counter, 0] = int(stroke[j][k][0]) - prev_x\n",
    "                    stroke_data[counter, 1] = int(stroke[j][k][1]) - prev_y\n",
    "                    prev_x = int(stroke[j][k][0])\n",
    "                    prev_y = int(stroke[j][k][1])\n",
    "                    stroke_data[counter, 2] = 0\n",
    "                    if (k == (len(stroke[j]) - 1)):  # end of stroke\n",
    "                        stroke_data[counter, 2] = 1\n",
    "                    counter += 1\n",
    "            return stroke_data\n",
    "\n",
    "        # build stroke database of every xml file inside iam database\n",
    "        strokes = []\n",
    "        for i in range(len(filelist)):\n",
    "            if (filelist[i][-3:] == 'xml'):\n",
    "                print('processing ' + filelist[i])\n",
    "                strokes.append(\n",
    "                    convert_stroke_to_array(\n",
    "                        getStrokes(\n",
    "                            filelist[i])))\n",
    "\n",
    "        f = open(data_file, \"wb\")\n",
    "        pickle.dump(strokes, f, protocol=2)\n",
    "        f.close()\n",
    "\n",
    "    def load_preprocessed(self, data_file):\n",
    "        f = open(data_file, \"rb\")\n",
    "        self.raw_data = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "        # goes thru the list, and only keeps the text entries that have more\n",
    "        # than seq_length points\n",
    "        self.data = []\n",
    "        self.valid_data = []\n",
    "        counter = 0\n",
    "\n",
    "        # every 1 in 20 (5%) will be used for validation data\n",
    "        cur_data_counter = 0\n",
    "        for data in self.raw_data:\n",
    "            if len(data) > (self.seq_length + 2):\n",
    "                # removes large gaps from the data\n",
    "                data = np.minimum(data, self.limit)\n",
    "                data = np.maximum(data, -self.limit)\n",
    "                data = np.array(data, dtype=np.float32)\n",
    "                data[:, 0:2] /= self.scale_factor\n",
    "                cur_data_counter = cur_data_counter + 1\n",
    "                if cur_data_counter % 20 == 0:\n",
    "                    self.valid_data.append(data)\n",
    "                else:\n",
    "                    self.data.append(data)\n",
    "                    # number of equiv batches this datapoint is worth\n",
    "                    counter += int(len(data) / ((self.seq_length + 2)))\n",
    "\n",
    "        print(\"train data: {}, valid data: {}\".format(\n",
    "            len(self.data), len(self.valid_data)))\n",
    "        # minus 1, since we want the ydata to be a shifted version of x data\n",
    "        self.num_batches = int(counter / self.batch_size)\n",
    "\n",
    "    def validation_data(self):\n",
    "        # returns validation data\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for i in range(self.batch_size):\n",
    "            data = self.valid_data[i % len(self.valid_data)]\n",
    "            idx = 0\n",
    "            x_batch.append(np.copy(data[idx:idx + self.seq_length]))\n",
    "            y_batch.append(np.copy(data[idx + 1:idx + self.seq_length + 1]))\n",
    "        return x_batch, y_batch\n",
    "\n",
    "    def next_batch(self):\n",
    "        # returns a randomised, seq_length sized portion of the training data\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for i in range(self.batch_size):\n",
    "            data = self.data[self.pointer]\n",
    "            # number of equiv batches this datapoint is worth\n",
    "            n_batch = int(len(data) / ((self.seq_length + 2)))\n",
    "            idx = random.randint(0, len(data) - self.seq_length - 2)\n",
    "            x_batch.append(np.copy(data[idx:idx + self.seq_length]))\n",
    "            y_batch.append(np.copy(data[idx + 1:idx + self.seq_length + 1]))\n",
    "            # adjust sampling probability.\n",
    "            if random.random() < (1.0 / float(n_batch)):\n",
    "                # if this is a long datapoint, sample this data more with\n",
    "                # higher probability\n",
    "                self.tick_batch_pointer()\n",
    "        return x_batch, y_batch\n",
    "\n",
    "    def tick_batch_pointer(self):\n",
    "        self.pointer += 1\n",
    "        if (self.pointer >= len(self.data)):\n",
    "            self.pointer = 0\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "This is where the fun begins. The network consists of LSTM cells stacked on top of each other and followed by a Gaussian mixture layer. This network does not include skip connections unlike the paper (see my other notebook for that : Model 2). \n",
    "\n",
    "`__init__(self, hidden_size = 256, n_layers = 3, n_gaussians = 10, dropout = 0)` :\n",
    "This is the constructor. It takes the different parameters to create the different blocks of the model.\n",
    "- hidden_size is the size of the hidden/output of each LSTM cell\n",
    "- n_layers is the number of LSTM cells stacked on top of each other\n",
    "- n_gaussians is the number of mixtures\n",
    "- dropout is the dropout probability. It gives the probability to skip a cell during forward propagation\n",
    "\n",
    "The Gaussian mixtures are created using a dense layer. It takes the output of the last LSTM layer. Say the hidden size is 256 and you want 10 mixtures, this allows to scale your vector to the desired size. This gives ŷ of equation 17 of the paper.\n",
    "![eq17](./pictures/eq17.png)\n",
    "ŷ is then broken down into the different parameters of the mixture. \n",
    "- ê is the probability of the end of a stroke given by a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution)\n",
    "- w (or $\\Pi$ ) is the weight of each Normal distribution\n",
    "- $\\mu, \\sigma, \\rho$ are the mean, standard deviation and correlation factor of each [bivariate Normal Distribution](http://mathworld.wolfram.com/BivariateNormalDistribution.html)\n",
    "The constructor juste lays out the blocks but does not create relations between them. That's the job of the forward function.\n",
    "\n",
    "\n",
    "`forward(self, x, hidden = None)` :\n",
    "This is the forward propagation. It takes x, a batch of dimensions [sequence_size, batch_size, 3]. The 3 corresponds to x and y offset of a stroke and eos = 1 when reaching an end of stroke (when the pen is raised). Note that the forward function is also used to generate random sequences.\n",
    "\n",
    "The first step is to compute the LSTM cells block. This is straightfoward in PyTorch. When hidden = none, the hidden and cell state vectors are zeros. \n",
    "\n",
    "Then it's just a matter of computing 18 - 22 of the paper.\n",
    "![eq18-22](./pictures/eq18-22.png)\n",
    "\n",
    "\n",
    "`generate_sequence(self, x0, sequence_length = 100)` :\n",
    "This is where I clearly sacrifice performance for readability. The goal of this function is to return a sequence based on either a single point or begining of sequence x0. In pseudo-code :\n",
    "- Calculte the mixture parameters of sequence x0\n",
    "- Pick a random mixture based on the weights (pi_idx)\n",
    "- Take a random point from the chosen bivariate normal distribution\n",
    "- Add it at the end of the sequence (concatenate it)\n",
    "- Repeat\n",
    "\n",
    "\n",
    "This clearly is bad practise as I have to rerun the forward prop on the entire sequence. And the sequence gets longer and longer which takes more time to compute at each new point generated. However this holds in just a few lines and keeps the forward function cleaner.\n",
    "\n",
    "\n",
    "`generate_sample(self, mu1, mu2, sigma1, sigma2, rho)` :\n",
    "Returns random coordinates based on a bivariate normal distribution given by the function parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "class HandwritingGenerationModel(nn.Module):\n",
    "    def __init__(self, hidden_size = 256, n_layers = 3, n_gaussians = 10, dropout = 0):\n",
    "        super(HandwritingGenerationModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_gaussians = n_gaussians\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = 3, hidden_size = hidden_size, num_layers = n_layers, dropout = dropout)\n",
    "        \n",
    "        self.z_e = nn.Linear(hidden_size, 1)\n",
    "        self.z_pi = nn.Linear(hidden_size, n_gaussians)\n",
    "        self.z_mu1 = nn.Linear(hidden_size, n_gaussians)\n",
    "        self.z_mu2 = nn.Linear(hidden_size, n_gaussians)\n",
    "        self.z_sigma1 = nn.Linear(hidden_size, n_gaussians)\n",
    "        self.z_sigma2 = nn.Linear(hidden_size, n_gaussians)\n",
    "        self.z_rho = nn.Linear(hidden_size, n_gaussians)\n",
    "        \n",
    "    def forward(self, x, hidden = None):\n",
    "        # number of batches\n",
    "        n_batch = x.shape[0]\n",
    "        \n",
    "        # Computing the LSTM cells block\n",
    "        out, hidden = self.lstm(x, hidden) \n",
    "        # print(\"out shape \", out.shape) # torch.Size([sequence_length, batch, hidden_size])\n",
    "\n",
    "        es = self.z_e(out)\n",
    "        # print(\"es shape \", es.shape) # -> torch.Size([sequence_length, batch, 1])\n",
    "        es = 1 / (1 + torch.exp(es))\n",
    "        # print(\"es shape\", es.shape) # -> torch.Size([sequence_length, batch, 1])\n",
    "\n",
    "        pis = self.z_pi(out)\n",
    "        # print(\"pis shape \", pis.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "        pis = torch.softmax(pis, 2)\n",
    "        # print(pi.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "\n",
    "        mu1s = self.z_mu1(out)\n",
    "        mu2s = self.z_mu2(out)\n",
    "        # print(\"mu shape :  \", mu1s.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "\n",
    "        sigma1s = self.z_sigma1(out)\n",
    "        sigma2s = self.z_sigma2(out)\n",
    "        # print(\"sigmas shape \", sigma1s.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "        sigma1s = torch.exp(sigma1s)\n",
    "        sigma2s = torch.exp(sigma2s)\n",
    "        # print(sigma1.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "\n",
    "        rhos = self.z_rho(out)\n",
    "        rhos = torch.tanh(rhos)\n",
    "        # print(\"rhos shape \", rhos.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "\n",
    "        es = es.squeeze(2) \n",
    "        # print(\"es shape \", es.shape) # -> torch.Size([sequence_length, batch])\n",
    "\n",
    "        \n",
    "        return es, pis, mu1s, mu2s, sigma1s, sigma2s, rhos\n",
    "    \n",
    "            \n",
    "    def generate_sample(self, mu1, mu2, sigma1, sigma2, rho):\n",
    "        mean = [mu1, mu2]\n",
    "        cov = [[sigma1 ** 2, rho * sigma1 * sigma2], [rho * sigma1 * sigma2, sigma2 ** 2]]\n",
    "        \n",
    "        x = np.float32(np.random.multivariate_normal(mean, cov, 1))\n",
    "        return torch.from_numpy(x)\n",
    "        \n",
    "        \n",
    "    def generate_sequence(self, x0, sequence_length = 100):\n",
    "        sequence = x0\n",
    "        \n",
    "        # A fun little widget\n",
    "        print(\"Generating sequence ...\")\n",
    "        f = FloatProgress(min=0, max=sequence_length)\n",
    "        display(f)\n",
    "        \n",
    "        for i in range(sequence_length):\n",
    "            es, pis, mu1s, mu2s, sigma1s, sigma2s, rhos = self.forward(sequence)\n",
    "            \n",
    "            # Selecting a mixture \n",
    "            pi_idx = np.random.choice(range(self.n_gaussians), p=pis[-1, 0, :].detach().cpu().numpy())\n",
    "            \n",
    "            # taking last parameters from sequence corresponding to chosen gaussian\n",
    "            mu1 = mu1s[-1, :, pi_idx].item()\n",
    "            mu2 = mu2s[-1, :, pi_idx].item()\n",
    "            sigma1 = sigma1s[-1, :, pi_idx].item()\n",
    "            sigma2 = sigma2s[-1, :, pi_idx].item()\n",
    "            rho = rhos[-1, :, pi_idx].item()\n",
    "            \n",
    "            prediction = self.generate_sample(mu1, mu2, sigma1, sigma2, rho)\n",
    "            eos = torch.distributions.bernoulli.Bernoulli(torch.tensor([es[-1, :].item()])).sample()\n",
    "            \n",
    "            sample = torch.zeros_like(x0) # torch.Size([1, 1, 3])\n",
    "            sample[0, 0, 0] = prediction[0, 0]\n",
    "            sample[0, 0, 1] = prediction[0, 1]\n",
    "            sample[0, 0, 2] = eos\n",
    "            \n",
    "            sequence = torch.cat((sequence, sample), 0) # torch.Size([sequence_length, 1, 3])\n",
    "            \n",
    "            f.value += 1\n",
    "        \n",
    "        return sequence.squeeze(1).detach().cpu().numpy()\n",
    "                     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing density probability\n",
    "\n",
    "It's time to implement the probability density of our next point given our output vector (the gaussian mixtures parameters). In the paper, this is given by equations 23-25. This will be useful when computing the loss function. \n",
    "\n",
    "![eq23-25](./pictures/eq23-25.png)\n",
    "\n",
    "I chose to exclude the Bernouilli part for now. It will be computed in the loss function.\n",
    "\n",
    "`gaussianMixture(y, pis, mu1s, mu2s, sigma1s, sigma2s, rhos)` :\n",
    "\n",
    "Remember the forward function of our model. gaussianMixture(...) takes as parameters its outputs. As such, it computes the results of equation 23 of the whole sequence over the different batches. A note on parameter y. It is basically the same tensor as x but shifted one time step. Think of it as $x_{t+1}$ in equation 23. It allows the last point of a sequence to still be learned correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussianMixture(y, pis, mu1s, mu2s, sigma1s, sigma2s, rhos):\n",
    "    n_mixtures = pis.size(2)\n",
    "    \n",
    "    # Takes x1 and repeats it over the number of gaussian mixtures\n",
    "    x1 = y[:,:, 0].repeat(n_mixtures, 1, 1).permute(1, 2, 0) \n",
    "    # print(\"x1 shape \", x1.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "    \n",
    "    # first term of Z (eq 25)\n",
    "    x1norm = ((x1 - mu1s) ** 2) / (sigma1s ** 2 )\n",
    "    # print(\"x1norm shape \", x1.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "    \n",
    "    x2 = y[:,:, 1].repeat(n_mixtures, 1, 1).permute(1, 2, 0)  \n",
    "    # print(\"x2 shape \", x2.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "    \n",
    "    # second term of Z (eq 25)\n",
    "    x2norm = ((x2 - mu2s) ** 2) / (sigma2s ** 2 )\n",
    "    # print(\"x2norm shape \", x2.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "    \n",
    "    # third term of Z (eq 25)\n",
    "    coxnorm = 2 * rhos * (x1 - mu1s) * (x2 - mu2s) / (sigma1s * sigma2s) \n",
    "    \n",
    "    # Computing Z (eq 25)\n",
    "    Z = x1norm + x2norm - coxnorm\n",
    "    \n",
    "    # Gaussian bivariate (eq 24)\n",
    "    N = torch.exp(-Z / (2 * (1 - rhos ** 2))) / (2 * np.pi * sigma1s * sigma2s * (1 - rhos ** 2) ** 0.5) \n",
    "    # print(\"N shape \", N.shape) # -> torch.Size([sequence_length, batch, n_gaussians]) \n",
    "    \n",
    "    # Pr is the result of eq 23 without the eos part\n",
    "    Pr = pis * N \n",
    "    # print(\"Pr shape \", Pr.shape) # -> torch.Size([sequence_length, batch, n_gaussians])   \n",
    "    Pr = torch.sum(Pr, dim=2) \n",
    "    # print(\"Pr shape \", Pr.shape) # -> torch.Size([sequence_length, batch])   \n",
    "    \n",
    "    if use_cuda:\n",
    "        Pr = Pr.cuda()\n",
    "    \n",
    "    return Pr\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing loss fn\n",
    "\n",
    "The goal is to maximize the likelihood of our estimated bivarate normal distributions and Bernoulli distribution. Think about it this way. We generate parameters for our distributions but we want them to fit as best as possible to our data. Each training step's goal is to converge toward the best parameters for our data. [Click here to read more about likelihood function](https://en.wikipedia.org/wiki/Likelihood_function).\n",
    "\n",
    "In the paper, the loss is given by equation 26 :\n",
    "\n",
    "![eq26](./pictures/eq26.png)\n",
    "\n",
    "We previously calculated the first element of the equation in gaussianMixture(...). What's left is to add the Bernoulli loss (second part of our equation). The loss of each time step is summed up and averaged over the batches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(Pr, y, es):\n",
    "    loss1 = - torch.log(Pr + eps) # -> torch.Size([sequence_length, batch])    \n",
    "    bernouilli = torch.zeros_like(es) # -> torch.Size([sequence_length, batch])\n",
    "    \n",
    "    bernouilli = y[:, :, 2] * es + (1 - y[:, :, 2]) * (1 - es)\n",
    "    \n",
    "    loss2 = - torch.log(bernouilli + eps)\n",
    "    loss = loss1 + loss2 \n",
    "    # print(\"loss shape\", loss.shape) # -> torch.Size([sequence_length, batch])  \n",
    "    loss = torch.sum(loss, 0) \n",
    "    # print(\"loss shape\", loss.shape) # -> torch.Size([batch]) \n",
    "    \n",
    "    return torch.mean(loss);\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The hardest part is behind us ! All that's left is to train our model. I used an Adam optimizer with a learning rate of 0.005. I haven't fiddled around too much with it as it already yields good resulsts. The gradients are clipped inside [-gradient_threshold, gradient_treshold] to avoid exploding gradient. A sequence is generated every 100 batches to see how the model is learning. Looks it works !\n",
    "\n",
    "![sample1](./pictures/sample1.png)\n",
    "![sample2](./pictures/sample2.png)\n",
    "\n",
    "The network is able to pick a style and stick with it. Of course it is unreadable but is is convincing enough.\n",
    "\n",
    "This is what it looks like training\n",
    "![training](./pictures/training_model1.png)\n",
    "\n",
    "And the loss function after 10 epochs. In orange is the average loss per epoch, in blue the loss per batch.\n",
    "![loss plot](./pictures/loss_graph_model1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, epochs = 5, generate = True):\n",
    "    data_loader = DataLoader(n_batch, sequence_length, 20) # 20 = datascale\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        \n",
    "    # Arrays to plot loss over time\n",
    "    time_batch = []\n",
    "    time_epoch = [0]\n",
    "    loss_batch = []\n",
    "    loss_epoch = []\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        data_loader.reset_batch_pointer()\n",
    "        \n",
    "        \n",
    "        for batch in range(data_loader.num_batches):\n",
    "            \n",
    "            \n",
    "            x, y = data_loader.next_batch()\n",
    "            x = np.float32(np.array(x)) # -> (n_batch, sequence_length, 3)\n",
    "            y = np.float32(np.array(y)) # -> (n_batch, sequence_length, 3)\n",
    "\n",
    "            x = torch.from_numpy(x).permute(1, 0, 2) # torch.Size([sequence_length, n_batch, 3])\n",
    "            y = torch.from_numpy(y).permute(1, 0, 2) # torch.Size([sequence_length, n_batch, 3])\n",
    "            \n",
    "            if use_cuda:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "            \n",
    "            # Forward pass\n",
    "            es, pis, mu1s, mu2s, sigma1s, sigma2s, rhos = model.forward(x)\n",
    "            \n",
    "            # Calculate probability density and loss\n",
    "            Pr = gaussianMixture(y, pis, mu1s, mu2s, sigma1s, sigma2s, rhos)\n",
    "            loss = loss_fn(Pr,y, es)\n",
    "            \n",
    "            # Back propagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient cliping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_threshold)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Useful infos\n",
    "            if batch % 100 == 0:\n",
    "                print(\"Epoch : \", epoch, \" - step \", batch, \"/\", data_loader.num_batches, \" - loss \", loss.item(), \" in \", time.time() - start)\n",
    "                start = time.time()\n",
    "            \n",
    "                if generate and batch % 500 == 0:\n",
    "                    x0 = torch.Tensor([0,0,1]).view(1,1,3)\n",
    "\n",
    "                    if use_cuda:\n",
    "                        x0 = x0.cuda()\n",
    "\n",
    "                    sequence = model.generate_sequence(x0, sequence_length = 300)\n",
    "                    draw_strokes_random_color(sequence, factor=0.5)\n",
    "                    \n",
    "                    \n",
    "            # Save loss per batch\n",
    "            time_batch.append(epoch + batch / data_loader.num_batches)\n",
    "            loss_batch.append(loss.item())\n",
    "        \n",
    "        # Save loss per epoch\n",
    "        time_epoch.append(epoch + 1)\n",
    "        loss_epoch.append(sum(loss_batch[epoch * data_loader.num_batches : (epoch + 1)*data_loader.num_batches-1]) / data_loader.num_batches)\n",
    "        \n",
    "        # Save model after each epoch\n",
    "        torch.save(model.state_dict(), \"./models/prediction_model1.py\")\n",
    "        \n",
    "    # Plot loss \n",
    "    plt.plot(time_batch, loss_batch)\n",
    "    plt.plot(time_epoch, [loss_batch[0]] + loss_epoch, color=\"orange\", linewidth=5)\n",
    "    plt.xlabel(\"Epoch\", fontsize=15)\n",
    "    plt.ylabel(\"Loss\", fontsize=15)\n",
    "    plt.show()\n",
    "        \n",
    "    return model, time_batch, loss_batch, time_epoch, [loss_batch[0]] + loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HandwritingGenerationModel(hidden_size, n_layers, n_gaussians, dropout)\n",
    "model, time_batch, loss_batch, time_epoch, loss_epoch = train_network(model, epochs=10, generate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some code to get started\n",
    "\n",
    "Below is some code to test the different features of the notebook !\n",
    "\n",
    "```Python\n",
    "data_loader = DataLoader(n_batch, sequence_length, 20) # 20 = datascale\n",
    "model = HandwritingGenerationModel(hidden_size, n_layers, n_gaussians, dropout)\n",
    "\n",
    "x, y = data_loader.next_batch()\n",
    "x = np.float32(np.array(x)) # -> (n_batch, sequence_length, 3)\n",
    "y = np.float32(np.array(y)) # -> (n_batch, sequence_length, 3)\n",
    "\n",
    "x = torch.from_numpy(x).permute(1, 0, 2) # torch.Size([sequence_length, n_batch, 3])\n",
    "y = torch.from_numpy(y).permute(1, 0, 2)\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "\n",
    "es, pis, mu1s, mu2s, sigma1s, sigma2s, rhos = model.forward(x)\n",
    "\n",
    "Pr = gaussianMixture(y, pis, mu1s, mu2s, sigma1s, sigma2s, rhos)\n",
    "loss = loss_fn(Pr,y, es)\n",
    "\n",
    "x0 = torch.Tensor([0,0,1]).view(1,1,3)\n",
    "\n",
    "if use_cuda:\n",
    "    x0 = x0.cuda()\n",
    "\n",
    "sequence = model.generate_sequence(x0, sequence_length = 300)\n",
    "draw_strokes_random_color(sequence, factor=0.5)\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
