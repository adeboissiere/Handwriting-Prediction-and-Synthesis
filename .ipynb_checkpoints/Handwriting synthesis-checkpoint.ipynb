{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwriting Synthesis\n",
    "This notebook is a personal attempt at coding Alex Graves RNN to predict handwriting (section 5). The paper can be found [here](https://arxiv.org/abs/1308.0850). It is almost the same as the handwriting prediction network (Handwriting prediction - Model 2) but incorporates an attention mechanism so the network learns what character it is writing.\n",
    "\n",
    "The goal of this notebook is to implement a network in a straightforward manner. As such, code readability is a priority over performance. The implemented network consists of layers of LSTM followed by a Gaussian mixtures layer, with an attention mechanism in between. Handwriting is highly variable. It makes more sense to generate a probability density function at each time step for the next stroke to capture that essence.\n",
    "\n",
    "The network is tweakable in sequence length, number of mixtures and dropout probability.\n",
    "\n",
    "The notebook is divided into data treatment (I used [Greydanus's code](https://nbviewer.jupyter.org/github/greydanus/scribe/blob/master/dataloader.ipynb) for that as that part is boring, a variation from hardmaru's code) some useful functions, network class, loss function and training. \n",
    "\n",
    "The dataset comes from [IAM On-Line Handwriting Database](http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database). Download data/lineStrokes-all.tar.gz after signing up ! The path should be ./data/lineStrokes if you want to use this notebook.\n",
    "\n",
    "Enjoy :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "import svgwrite\n",
    "from IPython.display import SVG, display\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "use_cuda = False\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "\n",
    "n_batch = 20\n",
    "sequence_length = 400\n",
    "U_items = int(sequence_length/25)\n",
    "\n",
    "hidden_size = 256\n",
    "n_layers = 3\n",
    "n_gaussians = 20\n",
    "Kmixtures = 10\n",
    "\n",
    "eps = float(np.finfo(np.float32).eps)\n",
    "\n",
    "# Hyperparameters\n",
    "gradient_threshold = 10\n",
    "dropout = 0.2\n",
    "\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "This code comes from [Greydanus](https://nbviewer.jupyter.org/github/greydanus/scribe/blob/master/dataloader.ipynb). Big thanks to his author !\n",
    "\n",
    "That part is not that fun. Dataloader is a class that parses all the .xml files. It creates a pickle file for future use. It creates a training set containing sequences x, y (same as x but shifted one timestep) and c (one-hot encoding of the sequence) in batches depending on the sequence length. Function `next_batch()` neatly returns a batch. Use `reset_batch_pointer()` to reset the current batch. See the training function for a proper use of that wonderful code. \n",
    "\n",
    "In this notebook, we won't use the hot-one vectors as it is used to implement the attention mechanism of section 5 of the paper.\n",
    "\n",
    "Some examples of training data :\n",
    "\n",
    "![batch2](./pictures/batch_model2.png)\n",
    "\n",
    "And some example code to load the data :\n",
    "\n",
    "```python\n",
    "x, y, s, c = data_loader.next_batch()\n",
    "print (data_loader.pointer)\n",
    "for i in range(n_batch):\n",
    "    r = x[i]\n",
    "    strokes = r.copy()\n",
    "    strokes[:,:-1] = np.cumsum(r[:,:-1], axis=0)\n",
    "    line_plot(strokes, s[i][:U_items])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounds(data, factor):\n",
    "    min_x = 0\n",
    "    max_x = 0\n",
    "    min_y = 0\n",
    "    max_y = 0\n",
    "\n",
    "    abs_x = 0\n",
    "    abs_y = 0\n",
    "    for i in range(len(data)):\n",
    "        x = float(data[i, 0]) / factor\n",
    "        y = float(data[i, 1]) / factor\n",
    "        abs_x += x\n",
    "        abs_y += y\n",
    "        min_x = min(min_x, abs_x)\n",
    "        min_y = min(min_y, abs_y)\n",
    "        max_x = max(max_x, abs_x)\n",
    "        max_y = max(max_y, abs_y)\n",
    "\n",
    "    return (min_x, max_x, min_y, max_y)\n",
    "\n",
    "# old version, where each path is entire stroke (smaller svg size, but\n",
    "# have to keep same color)\n",
    "\n",
    "\n",
    "def draw_strokes(data, factor=10, svg_filename='sample.svg'):\n",
    "    min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
    "    dims = (50 + max_x - min_x, 50 + max_y - min_y)\n",
    "\n",
    "    dwg = svgwrite.Drawing(svg_filename, size=dims)\n",
    "    dwg.add(dwg.rect(insert=(0, 0), size=dims, fill='white'))\n",
    "\n",
    "    lift_pen = 1\n",
    "\n",
    "    abs_x = 25 - min_x\n",
    "    abs_y = 25 - min_y\n",
    "    p = \"M%s,%s \" % (abs_x, abs_y)\n",
    "\n",
    "    command = \"m\"\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if (lift_pen == 1):\n",
    "            command = \"m\"\n",
    "        elif (command != \"l\"):\n",
    "            command = \"l\"\n",
    "        else:\n",
    "            command = \"\"\n",
    "        x = float(data[i, 0]) / factor\n",
    "        y = float(data[i, 1]) / factor\n",
    "        lift_pen = data[i, 2]\n",
    "        p += command + str(x) + \",\" + str(y) + \" \"\n",
    "\n",
    "    the_color = \"black\"\n",
    "    stroke_width = 1\n",
    "\n",
    "    dwg.add(dwg.path(p).stroke(the_color, stroke_width).fill(\"none\"))\n",
    "\n",
    "    dwg.save()\n",
    "    display(SVG(dwg.tostring()))\n",
    "\n",
    "\n",
    "def draw_strokes_eos_weighted(\n",
    "        stroke,\n",
    "        param,\n",
    "        factor=10,\n",
    "        svg_filename='sample_eos.svg'):\n",
    "    c_data_eos = np.zeros((len(stroke), 3))\n",
    "    for i in range(len(param)):\n",
    "        # make color gray scale, darker = more likely to eos\n",
    "        c_data_eos[i, :] = (1 - param[i][6][0]) * 225\n",
    "    draw_strokes_custom_color(\n",
    "        stroke,\n",
    "        factor=factor,\n",
    "        svg_filename=svg_filename,\n",
    "        color_data=c_data_eos,\n",
    "        stroke_width=3)\n",
    "\n",
    "\n",
    "def draw_strokes_random_color(\n",
    "        stroke,\n",
    "        factor=10,\n",
    "        svg_filename='sample_random_color.svg',\n",
    "        per_stroke_mode=True):\n",
    "    c_data = np.array(np.random.rand(len(stroke), 3) * 240, dtype=np.uint8)\n",
    "    if per_stroke_mode:\n",
    "        switch_color = False\n",
    "        for i in range(len(stroke)):\n",
    "            if switch_color == False and i > 0:\n",
    "                c_data[i] = c_data[i - 1]\n",
    "            if stroke[i, 2] < 1:  # same strike\n",
    "                switch_color = False\n",
    "            else:\n",
    "                switch_color = True\n",
    "    draw_strokes_custom_color(\n",
    "        stroke,\n",
    "        factor=factor,\n",
    "        svg_filename=svg_filename,\n",
    "        color_data=c_data,\n",
    "        stroke_width=2)\n",
    "\n",
    "\n",
    "def draw_strokes_custom_color(\n",
    "        data,\n",
    "        factor=10,\n",
    "        svg_filename='test.svg',\n",
    "        color_data=None,\n",
    "        stroke_width=1):\n",
    "    min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
    "    dims = (50 + max_x - min_x, 50 + max_y - min_y)\n",
    "\n",
    "    dwg = svgwrite.Drawing(svg_filename, size=dims)\n",
    "    dwg.add(dwg.rect(insert=(0, 0), size=dims, fill='white'))\n",
    "\n",
    "    lift_pen = 1\n",
    "    abs_x = 25 - min_x\n",
    "    abs_y = 25 - min_y\n",
    "\n",
    "    for i in range(len(data)):\n",
    "\n",
    "        x = float(data[i, 0]) / factor\n",
    "        y = float(data[i, 1]) / factor\n",
    "\n",
    "        prev_x = abs_x\n",
    "        prev_y = abs_y\n",
    "\n",
    "        abs_x += x\n",
    "        abs_y += y\n",
    "\n",
    "        if (lift_pen == 1):\n",
    "            p = \"M \" + str(abs_x) + \",\" + str(abs_y) + \" \"\n",
    "        else:\n",
    "            p = \"M +\" + str(prev_x) + \",\" + str(prev_y) + \\\n",
    "                \" L \" + str(abs_x) + \",\" + str(abs_y) + \" \"\n",
    "\n",
    "        lift_pen = data[i, 2]\n",
    "\n",
    "        the_color = \"black\"\n",
    "\n",
    "        if (color_data is not None):\n",
    "            the_color = \"rgb(\" + str(int(color_data[i, 0])) + \",\" + str(\n",
    "                int(color_data[i, 1])) + \",\" + str(int(color_data[i, 2])) + \")\"\n",
    "\n",
    "        dwg.add(dwg.path(p).stroke(the_color, stroke_width).fill(the_color))\n",
    "    dwg.save()\n",
    "    display(SVG(dwg.tostring()))\n",
    "\n",
    "        \n",
    "class DataLoader():\n",
    "    def __init__(self, batch_size=50, tsteps=300, scale_factor = 10, U_items=10, limit = 500, alphabet=\"default\"):\n",
    "        self.data_dir = \"./data\"\n",
    "        self.alphabet = alphabet\n",
    "        self.batch_size = batch_size\n",
    "        self.tsteps = tsteps\n",
    "        self.scale_factor = scale_factor # divide data by this factor\n",
    "        self.limit = limit # removes large noisy gaps in the data\n",
    "        self.U_items = U_items\n",
    "\n",
    "        data_file = os.path.join(self.data_dir, \"strokes_training_data_generation.cpkl\")\n",
    "        stroke_dir = self.data_dir+\"/lineStrokes\"\n",
    "        ascii_dir = self.data_dir+\"/ascii\"\n",
    "\n",
    "        if not (os.path.exists(data_file)) :\n",
    "            print (\"creating training data cpkl file from raw source\")\n",
    "            self.preprocess(stroke_dir, ascii_dir, data_file)\n",
    "\n",
    "        self.load_preprocessed(data_file)\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, stroke_dir, ascii_dir, data_file):\n",
    "        # create data file from raw xml files from iam handwriting source.\n",
    "        print (\"Parsing dataset...\")\n",
    "        \n",
    "        # build the list of xml files\n",
    "        filelist = []\n",
    "        # Set the directory you want to start from\n",
    "        rootDir = stroke_dir\n",
    "        for dirName, subdirList, fileList in os.walk(rootDir):\n",
    "#             print('Found directory: %s' % dirName)\n",
    "            for fname in fileList:\n",
    "#                 print('\\t%s' % fname)\n",
    "                filelist.append(dirName+\"/\"+fname)\n",
    "\n",
    "        # function to read each individual xml file\n",
    "        def getStrokes(filename):\n",
    "            tree = ET.parse(filename)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            result = []\n",
    "\n",
    "            x_offset = 1e20\n",
    "            y_offset = 1e20\n",
    "            y_height = 0\n",
    "            for i in range(1, 4):\n",
    "                x_offset = min(x_offset, float(root[0][i].attrib['x']))\n",
    "                y_offset = min(y_offset, float(root[0][i].attrib['y']))\n",
    "                y_height = max(y_height, float(root[0][i].attrib['y']))\n",
    "            y_height -= y_offset\n",
    "            x_offset -= 100\n",
    "            y_offset -= 100\n",
    "\n",
    "            for stroke in root[1].findall('Stroke'):\n",
    "                points = []\n",
    "                for point in stroke.findall('Point'):\n",
    "                    points.append([float(point.attrib['x'])-x_offset,float(point.attrib['y'])-y_offset])\n",
    "                result.append(points)\n",
    "            return result\n",
    "        \n",
    "        # function to read each individual xml file\n",
    "        def getAscii(filename, line_number):\n",
    "            with open(filename, \"r\") as f:\n",
    "                s = f.read()\n",
    "            s = s[s.find(\"CSR\"):]\n",
    "            if len(s.split(\"\\n\")) > line_number+2:\n",
    "                s = s.split(\"\\n\")[line_number+2]\n",
    "                return s\n",
    "            else:\n",
    "                return \"\"\n",
    "                \n",
    "        # converts a list of arrays into a 2d numpy int16 array\n",
    "        def convert_stroke_to_array(stroke):\n",
    "            n_point = 0\n",
    "            for i in range(len(stroke)):\n",
    "                n_point += len(stroke[i])\n",
    "            stroke_data = np.zeros((n_point, 3), dtype=np.int16)\n",
    "\n",
    "            prev_x = 0\n",
    "            prev_y = 0\n",
    "            counter = 0\n",
    "\n",
    "            for j in range(len(stroke)):\n",
    "                for k in range(len(stroke[j])):\n",
    "                    stroke_data[counter, 0] = int(stroke[j][k][0]) - prev_x\n",
    "                    stroke_data[counter, 1] = int(stroke[j][k][1]) - prev_y\n",
    "                    prev_x = int(stroke[j][k][0])\n",
    "                    prev_y = int(stroke[j][k][1])\n",
    "                    stroke_data[counter, 2] = 0\n",
    "                    if (k == (len(stroke[j])-1)): # end of stroke\n",
    "                        stroke_data[counter, 2] = 1\n",
    "                    counter += 1\n",
    "            return stroke_data\n",
    "\n",
    "        # build stroke database of every xml file inside iam database\n",
    "        strokes = []\n",
    "        asciis = []\n",
    "        for i in range(len(filelist)):\n",
    "            if (filelist[i][-3:] == 'xml'):\n",
    "                stroke_file = filelist[i]\n",
    "#                 print 'processing '+stroke_file\n",
    "                stroke = convert_stroke_to_array(getStrokes(stroke_file))\n",
    "                \n",
    "                ascii_file = stroke_file.replace(\"lineStrokes\",\"ascii\")[:-7] + \".txt\"\n",
    "                line_number = stroke_file[-6:-4]\n",
    "                line_number = int(line_number) - 1\n",
    "                ascii = getAscii(ascii_file, line_number)\n",
    "                if len(ascii) > 10:\n",
    "                    strokes.append(stroke)\n",
    "                    asciis.append(ascii)\n",
    "                else:\n",
    "                    print (\"======>>>> Line length was too short. Line was: \" + ascii)\n",
    "                \n",
    "        assert(len(strokes)==len(asciis)), \"There should be a 1:1 correspondence between stroke data and ascii labels.\"\n",
    "        f = open(data_file,\"wb\")\n",
    "        pickle.dump([strokes,asciis], f, protocol=2)\n",
    "        f.close()\n",
    "        print (\"Finished parsing dataset. Saved {} lines\".format(len(strokes)))\n",
    "\n",
    "\n",
    "    def load_preprocessed(self, data_file):\n",
    "        f = open(data_file,\"rb\")\n",
    "        [self.raw_stroke_data, self.raw_ascii_data] = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "        # goes thru the list, and only keeps the text entries that have more than tsteps points\n",
    "        self.stroke_data = []\n",
    "        self.ascii_data = []\n",
    "        counter = 0\n",
    "\n",
    "        for i in range(len(self.raw_stroke_data)):\n",
    "            data = self.raw_stroke_data[i]\n",
    "            if len(data) > (self.tsteps+2):\n",
    "                # removes large gaps from the data\n",
    "                data = np.minimum(data, self.limit)\n",
    "                data = np.maximum(data, -self.limit)\n",
    "                data = np.array(data,dtype=np.float32)\n",
    "                data[:,0:2] /= self.scale_factor\n",
    "                \n",
    "                self.stroke_data.append(data)\n",
    "                self.ascii_data.append(self.raw_ascii_data[i])\n",
    "\n",
    "        # minus 1, since we want the ydata to be a shifted version of x data\n",
    "        self.num_batches = int(len(self.stroke_data) / self.batch_size)\n",
    "        print (\"Loaded dataset:\")\n",
    "        print (\"   -> {} individual data points\".format(len(self.stroke_data)))\n",
    "        print (\"   -> {} batches\".format(self.num_batches))\n",
    "\n",
    "    def next_batch(self):\n",
    "        # returns a randomised, tsteps sized portion of the training data\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        ascii_list = []\n",
    "        for i in range(self.batch_size):\n",
    "            data = self.stroke_data[self.idx_perm[self.pointer]]\n",
    "            x_batch.append(np.copy(data[:self.tsteps]))\n",
    "            y_batch.append(np.copy(data[1:self.tsteps+1]))\n",
    "            ascii_list.append(self.ascii_data[self.idx_perm[self.pointer]])\n",
    "            self.tick_batch_pointer()\n",
    "        one_hots = [self.one_hot(s) for s in ascii_list]\n",
    "        return x_batch, y_batch, ascii_list, one_hots\n",
    "    \n",
    "    def one_hot(self, s):\n",
    "        #index position 0 means \"unknown\"\n",
    "        if self.alphabet is \"default\":\n",
    "            alphabet = \" abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890\"\n",
    "        seq = [alphabet.find(char) + 1 for char in s]\n",
    "        if len(seq) >= self.U_items:\n",
    "            seq = seq[:self.U_items]\n",
    "        else:\n",
    "            seq = seq + [0]*(self.U_items - len(seq))\n",
    "        one_hot = np.zeros((self.U_items,len(alphabet)+1))\n",
    "        one_hot[np.arange(self.U_items),seq] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def tick_batch_pointer(self):\n",
    "        self.pointer += 1\n",
    "        if (self.pointer >= len(self.stroke_data)):\n",
    "            self.reset_batch_pointer()\n",
    "    def reset_batch_pointer(self):\n",
    "        self.idx_perm = np.random.permutation(len(self.stroke_data))\n",
    "        self.pointer = 0\n",
    "        print (\"pointer reset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "Here are a couple of useful functions\n",
    "\n",
    "`line_plot(strokes, title)`: plots a sequence. Results look like this :\n",
    "\n",
    "![batch2](./pictures/batch_model2.png)\n",
    "\n",
    "`one_hot(s)`: transforms a string sequence into a one-hot matrix. Dimensions of the output one-hot matrix are (string length, len(alphabet)). For example :\n",
    "\n",
    "```python\n",
    "s = \"writing is hard\"\n",
    "print(one_hot(s))\n",
    "```\n",
    "\n",
    "`plot_heatmaps(Phis, Ws)`: plots Phis and soft-window heatmaps. It corresponds to the values of equations 46 and 47 of the paper. \n",
    "\n",
    "![heatmpas](./pictures/heatmaps.png)\n",
    "![heatmpas](./pictures/eq46-47.png)\n",
    "\n",
    "`def get_n_params(model)`: returns the number of parameters of a model\n",
    "\n",
    "And some example code to load the data :\n",
    "\n",
    "```python\n",
    "x, y, s, c = data_loader.next_batch()\n",
    "print (data_loader.pointer)\n",
    "for i in range(n_batch):\n",
    "    r = x[i]\n",
    "    strokes = r.copy()\n",
    "    strokes[:,:-1] = np.cumsum(r[:,:-1], axis=0)\n",
    "    line_plot(strokes, s[i][:U_items])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_plot(strokes, title):\n",
    "    plt.figure(figsize=(20,2))\n",
    "    eos_preds = np.where(strokes[:,-1] == 1)\n",
    "    eos_preds = [0] + list(eos_preds[0]) + [-1] #add start and end indices\n",
    "    for i in range(len(eos_preds)-1):\n",
    "        start = eos_preds[i]+1\n",
    "        stop = eos_preds[i+1]\n",
    "        plt.plot(strokes[start:stop,0], strokes[start:stop,1],'b-', linewidth=2.0)\n",
    "    plt.title(title)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "def one_hot(s):\n",
    "    #index position 0 means \"unknown\"\n",
    "    alphabet = \" abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890\"\n",
    "    seq = [alphabet.find(char) + 1 for char in s]\n",
    "\n",
    "    one_hot = np.zeros((len(s),len(alphabet)+1))\n",
    "    one_hot[np.arange(len(s)),seq] = 1\n",
    "    return one_hot\n",
    "\n",
    "def plot_heatmaps(Phis, Ws):\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.subplot(121)\n",
    "    plt.title('Phis', fontsize=20)\n",
    "    plt.xlabel(\"time steps\", fontsize=15)\n",
    "    plt.ylabel(\"ascii #\", fontsize=15)\n",
    "    \n",
    "    plt.imshow(Phis, interpolation='nearest', aspect='auto', cmap=cm.jet)\n",
    "    plt.subplot(122)\n",
    "    plt.title('Soft attention window', fontsize=20)\n",
    "    plt.xlabel(\"time steps\", fontsize=15)\n",
    "    plt.ylabel(\"one-hot vector\", fontsize=15)\n",
    "    plt.imshow(Ws, interpolation='nearest', aspect='auto', cmap=cm.jet)\n",
    "\n",
    "    display(plt.gcf())\n",
    "    \n",
    "\n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "This is where the fun begins. The network consists of LSTM cells stacked on top of each other and followed by a Gaussian mixture layer with an attention mechanism in between. This network includes skip connections like the paper. It is almost the same network as the one in my other notebook (Handwriting prediction - Model 2) but with the attention mechanism.\n",
    "\n",
    "\n",
    "`__init__(self, hidden_size = 256, n_gaussians = 10, Kmixtures = 10, dropout = 0.2, alphabet_size = 64)` :\n",
    "This is the constructor. It takes the different parameters to create the different blocks of the model.\n",
    "- hidden_size is the size of the output of each LSTM cell\n",
    "- n_gaussians is the number of mixtures\n",
    "- Kmixtures is the number of Gaussian functions for the window vectors\n",
    "- dropout is the dropout probability. It gives the probability to skip a cell during forward propagation. It's not implemented actually\n",
    "- alphabet_size is the number of characters in our dictionary\n",
    "\n",
    "The attention mechanisme is implemented between LSTM1 and LSTM2. LSTM1 takes as inputs the window vectors of the previous time step as well as current stroke coordinates. A dense layer is used taking the output of LSTM1 to compute the parameters of the window vectors. The current window vector is passed on to LSTM2 and LSTM3 as well as the stroke coordinates via skip connections. LSTM2 and LSTM3 of course take the hidden vectors of the LSTM1 and LSTM2 respectively. This is summarized by equations 52 and 53.\n",
    "\n",
    "![eq52-53](./pictures/eq52-53.png)\n",
    "\n",
    "The Gaussian mixtures are created using a dense layer. It takes the output of the last LSTM layer. Say the hidden size is 256 and you want 10 mixtures, this allows to scale your vector to the desired size. This gives ŷ of equation 17 of the paper.\n",
    "\n",
    "![eq17](./pictures/eq17.png)\n",
    "\n",
    "ŷ is then broken down into the different parameters of the mixture. \n",
    "- ê is the probability of the end of a stroke given by a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution)\n",
    "- w (or $\\Pi$ ) is the weight of each Normal distribution\n",
    "- $\\mu, \\sigma, \\rho$ are the mean, standard deviation and correlation factor of each [bivariate Normal Distribution](http://mathworld.wolfram.com/BivariateNormalDistribution.html)\n",
    "The constructor juste lays out the blocks but does not create relations between them. That's the job of the forward function.\n",
    "\n",
    "The network is summarized by figure 12 (it does show the third hidden layer which does pretty much the same thing as the second).\n",
    "\n",
    "![figure12](./pictures/figure12.png)\n",
    "\n",
    "\n",
    "\n",
    "`forward(self, x, c)` :\n",
    "This is the forward propagation. It takes x and c as inputs. \n",
    "\n",
    "x is a batch of stroke coordinates of sequences. Its dimensions are [sequence_size, batch_size, 3]. The 3 corresponds to x and y offset of a stroke and eos (= 1 when reaching an end of stroke (when the pen is raised)). \n",
    "\n",
    "c, a batch of one-hot encoded sentences corresponding to the stroke sequence is of dimensions [n_batch, U_items, len(alphabet)]. It is estimated that a letter corresponds to 25 points. U_items is the number of characters in the sequence. For example, if the sequence is 400 points long, U_items = 400 / 25 = 16 charcaters. len(alphabet) is the number of characters in our alphabet.\n",
    "\n",
    "Note that the forward function is also used to generate random sequences.\n",
    "\n",
    "The first step is to compute LSTM1. This is straightfoward in PyTorch. Since I created LSTM cells in Pytorch. I need a for loop over the whole stroke sequence.\n",
    "\n",
    "After LSTM1, the code computes the attention mechanism given by equations 46-51 of the paper.\n",
    "\n",
    "![eq46-47](./pictures/eq46-47.png)\n",
    "![eq48-51](./pictures/eq48-51.png)\n",
    "\n",
    "After that, the networks computes LSTM2 and LSTM3. Then it's just a matter of computing 18 - 22 of the paper using a dense layer.\n",
    "![eq18-22](./pictures/eq18-22.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "`generate_sequence(self, x0, c0)` :\n",
    "This is where I clearly sacrifice performance for readability. The goal of this function is to return a sequence based on either a single point or begining of sequence x0. In pseudo-code :\n",
    "- Calculte the mixture parameters of sequence x0 given one-hot encoded string c0\n",
    "- Pick a random mixture based on the weights (pi_idx)\n",
    "- Take a random point from the chosen bivariate normal distribution\n",
    "- Add it at the end of the sequence (concatenate it)\n",
    "- Repeat\n",
    "\n",
    "\n",
    "This clearly is bad practise as I have to rerun the forward prop on the entire sequence each time. And the sequence gets longer and longer which takes more time to compute at each new point generated. However this holds in just a few lines and keeps the forward function cleaner.\n",
    "\n",
    "\n",
    "`generate_sample(self, mu1, mu2, sigma1, sigma2, rho)` :\n",
    "Returns random coordinates based on a bivariate normal distribution given by the function parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandwritingSynthesisModel(nn.Module):\n",
    "    def __init__(self, hidden_size = 256, n_gaussians = 20, Kmixtures = 10, dropout = 0.2, alphabet_size = 64):\n",
    "        super(HandwritingSynthesisModel, self).__init__()\n",
    "        \n",
    "        self.Kmixtures = Kmixtures\n",
    "        self.n_gaussians = n_gaussians\n",
    "        self.alphabet_size = alphabet_size\n",
    "        \n",
    "        self.hidden_size1 = hidden_size\n",
    "        self.hidden_size2 = hidden_size\n",
    "        self.hidden_size3 = hidden_size\n",
    "        \n",
    "        # input_size1 includes x, y, eos and len(w_t_1) given by alphabet_size (see eq 52)\n",
    "        self.input_size1 = 3 + alphabet_size\n",
    "        \n",
    "        # input_size2 includes x, y, eos, len(w_t) given by alphabet_size (see eq 47) and hidden_size1\n",
    "        self.input_size2 = 3 + alphabet_size + self.hidden_size1\n",
    "        \n",
    "        # input_size3 includes x, y, eos, len(w_t) given by alphabet_size (see eq 47) and hidden_size2\n",
    "        self.input_size3 = 3 + alphabet_size + self.hidden_size2\n",
    "        \n",
    "        # See eq 52-53 to understand the input_sizes\n",
    "        self.lstm1 = nn.LSTMCell(input_size= self.input_size1 , hidden_size = self.hidden_size1)\n",
    "        self.lstm2 = nn.LSTMCell(input_size= self.input_size2 , hidden_size = self.hidden_size2)\n",
    "        self.lstm3 = nn.LSTMCell(input_size= self.input_size3 , hidden_size = self.hidden_size3)\n",
    "        \n",
    "        # Initialize weights of LSTM cells\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(param, mean=0, std=0.075)\n",
    "        \n",
    "        # Window layer takes hidden layer of LSTM1 as input and outputs 3 * Kmixtures vectors\n",
    "        self.window_layer = nn.Linear(self.hidden_size1, 3 * Kmixtures)\n",
    "        \n",
    "        # Initialize weights of soft window dense layer\n",
    "        for name, param in self.window_layer.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.normal_(param, mean=-4, std=0.1)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.normal_(param, mean=0, std=0.075)\n",
    "        \n",
    "        # For gaussian mixtures\n",
    "        self.z_e = nn.Linear(hidden_size, 1)\n",
    "        self.z_pi = nn.Linear(hidden_size, n_gaussians)\n",
    "        self.z_mu1 = nn.Linear(hidden_size, n_gaussians)\n",
    "        self.z_mu2 = nn.Linear(hidden_size, n_gaussians)\n",
    "        self.z_sigma1 = nn.Linear(hidden_size, n_gaussians)\n",
    "        self.z_sigma2 = nn.Linear(hidden_size, n_gaussians)\n",
    "        self.z_rho = nn.Linear(hidden_size, n_gaussians)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, c):\n",
    "        # sequence length\n",
    "        sequence_length = x.shape[0]\n",
    "        \n",
    "        # number of batches\n",
    "        n_batch = x.shape[1]\n",
    "        \n",
    "        # Soft window vector w at t-1\n",
    "        w_t_1 = torch.ones(n_batch, self.alphabet_size) # torch.Size([n_batch, len(alphabet)])\n",
    "        \n",
    "        # Hidden and cell state for LSTM1\n",
    "        h1_t = torch.zeros(n_batch, self.hidden_size1) # torch.Size([n_batch, hidden_size1])\n",
    "        c1_t = torch.zeros(n_batch, self.hidden_size1) # torch.Size([n_batch, hidden_size1])\n",
    "        \n",
    "        # Kappa at t-1\n",
    "        kappa_t_1 = torch.zeros(n_batch, Kmixtures) # torch.Size([n_batch, Kmixtures])\n",
    "        \n",
    "        # Hidden and cell state for LSTM2\n",
    "        h2_t = torch.zeros(n_batch, self.hidden_size2) # torch.Size([n_batch, hidden_size2])\n",
    "        c2_t = torch.zeros(n_batch, self.hidden_size2) # torch.Size([n_batch, hidden_size2])\n",
    "        \n",
    "        # Hidden and cell state for LSTM3\n",
    "        h3_t = torch.zeros(n_batch, self.hidden_size3) # torch.Size([n_batch, hidden_size3])\n",
    "        c3_t = torch.zeros(n_batch, self.hidden_size3) # torch.Size([n_batch, hidden_size3])\n",
    "        \n",
    "        out = torch.zeros(sequence_length, n_batch, self.hidden_size3)\n",
    "        \n",
    "        # Phis and Ws allow to plot heatmaps of phi et w over time\n",
    "        self.Phis = torch.zeros(sequence_length, U_items)\n",
    "        self.Ws = torch.zeros(sequence_length, self.alphabet_size)\n",
    "        \n",
    "        if use_cuda:\n",
    "            w_t_1 = w_t_1.cuda()\n",
    "            \n",
    "            h1_t = h1_t.cuda()\n",
    "            c1_t = c1_t.cuda()\n",
    "            \n",
    "            kappa_t_1 = kappa_t_1.cuda()\n",
    "            \n",
    "            h2_t = h2_t.cuda()\n",
    "            c2_t = c2_t.cuda()\n",
    "            \n",
    "            h3_t = h3_t.cuda()\n",
    "            c3_t = c3_t.cuda()\n",
    "            \n",
    "            out = out.cuda()\n",
    "            \n",
    "        for i in range(sequence_length):\n",
    "            # ===== Computing 1st layer =====\n",
    "            input_lstm1 = torch.cat((x[i], w_t_1), 1) # torch.Size([n_batch, input_size1])\n",
    "            h1_t, c1_t = self.lstm1(input_lstm1, (h1_t, c1_t)) # torch.Size([n_batch, hidden_size1])\n",
    "            \n",
    "            # ===== Computing soft window =====\n",
    "            window = self.window_layer(h1_t)\n",
    "            \n",
    "            # splits exp(window) into 3 tensors of torch.Size([n_batch, Kmixtures])\n",
    "            # Eqs 48-51 of the paper\n",
    "            alpha_t, beta_t, kappa_t = torch.chunk( torch.exp(window), 3, dim=1) \n",
    "            kappa_t += kappa_t_1\n",
    "            \n",
    "            # updates kappa_t_1 for next iteration\n",
    "            kappa_t_1 = kappa_t\n",
    "            \n",
    "            u = torch.arange(0,U_items, out=kappa_t.new()).view(-1,1,1) # torch.Size([U_items, 1, 1])\n",
    "            \n",
    "            # Computing Phi(t, u)\n",
    "            # Eq 46 of the paper\n",
    "            # Keep in mind the (kappa_t - u).shape is torch.Size([U_items, n_batch, Kmixtures])\n",
    "            # For example :\n",
    "            ## (kappa_t - u)[0, 0, :] gives kappa_t[0, :]\n",
    "            ## (kappa_t - u)[1, 0, :] gives kappa_t[0, :] - 1\n",
    "            ## etc\n",
    "            Phi = alpha_t * torch.exp(- beta_t * (kappa_t - u) ** 2) # torch.Size([U_items, n_batch, Kmixtures])\n",
    "            Phi = torch.sum(Phi, dim = 2) # torch.Size([U_items, n_batch])  \n",
    "            Phi = torch.unsqueeze(Phi, 0) # torch.Size([1, U_items, n_batch])\n",
    "            Phi = Phi.permute(2, 0, 1) # torch.Size([n_batch, 1, U_items])\n",
    "            \n",
    "            self.Phis[i, :] = Phi[0, 0, :] # To plot heatmaps\n",
    "            \n",
    "            # Computing wt \n",
    "            # Eq 47 of the paper\n",
    "            w_t = torch.matmul(Phi, c) # torch.Size([n_batch, 1, len(alphabet)])\n",
    "            w_t = torch.squeeze(w_t, 1) # torch.Size([n_batch, len(alphabet)])\n",
    "            \n",
    "            self.Ws[i, :] = w_t[0, :] # To plot heatmaps\n",
    "            \n",
    "            # Update w_t_1 for next iteration\n",
    "            w_t_1 = w_t\n",
    "            \n",
    "            # ===== Computing 2nd layer =====\n",
    "            input_lstm2 = torch.cat((x[i], w_t, h1_t), 1) # torch.Size([n_batch, 3 + alphabet_size + hidden_size1])\n",
    "            h2_t, c2_t = self.lstm2(input_lstm2, (h2_t, c2_t)) \n",
    "            \n",
    "            \n",
    "            # ===== Computing 3rd layer =====\n",
    "            input_lstm3 = torch.cat((x[i], w_t, h2_t), 1) # torch.Size([n_batch, 3 + alphabet_size + hidden_size2])\n",
    "            h3_t, c3_t = self.lstm2(input_lstm2, (h3_t, c3_t))\n",
    "            out[i, :, :] = h3_t\n",
    "            \n",
    "        # ===== Computing MDN =====\n",
    "        es = self.z_e(out)\n",
    "        # print(\"es shape \", es.shape) # -> torch.Size([sequence_length, batch, 1])\n",
    "        es = 1 / (1 + torch.exp(es))\n",
    "        # print(\"es shape\", es.shape) # -> torch.Size([sequence_length, batch, 1])\n",
    "\n",
    "        pis = self.z_pi(out)\n",
    "        # print(\"pis shape \", pis.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "        pis = torch.softmax(pis, 2)\n",
    "        # print(pis.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "\n",
    "        mu1s = self.z_mu1(out)\n",
    "        mu2s = self.z_mu2(out)\n",
    "        # print(\"mu shape :  \", mu1s.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "\n",
    "        sigma1s = self.z_sigma1(out)\n",
    "        sigma2s = self.z_sigma2(out)\n",
    "        # print(\"sigmas shape \", sigma1s.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "        sigma1s = torch.exp(sigma1s)\n",
    "        sigma2s = torch.exp(sigma2s)\n",
    "        # print(sigma1s.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "\n",
    "        rhos = self.z_rho(out)\n",
    "        rhos = torch.tanh(rhos)\n",
    "        # print(\"rhos shape \", rhos.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "\n",
    "        es = es.squeeze(2) \n",
    "        # print(\"es shape \", es.shape) # -> torch.Size([sequence_length, batch])\n",
    "\n",
    "\n",
    "        return es, pis, mu1s, mu2s, sigma1s, sigma2s, rhos\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def generate_sample(self, mu1, mu2, sigma1, sigma2, rho):\n",
    "        mean = [mu1, mu2]\n",
    "        cov = [[sigma1 ** 2, rho * sigma1 * sigma2], [rho * sigma1 * sigma2, sigma2 ** 2]]\n",
    "        \n",
    "        x = np.float32(np.random.multivariate_normal(mean, cov, 1))\n",
    "        return torch.from_numpy(x)\n",
    "        \n",
    "        \n",
    "    def generate_sequence(self, x0, c0):\n",
    "        sequence = x0\n",
    "        sequence_length = c0.shape[1] * 25\n",
    "        \n",
    "        print(\"Generating sequence ...\")\n",
    "        f = FloatProgress(min=0, max=sequence_length)\n",
    "        display(f)\n",
    "\n",
    "        for i in range(sequence_length):\n",
    "            es, pis, mu1s, mu2s, sigma1s, sigma2s, rhos = self.forward(sequence, c0)\n",
    "            \n",
    "            # Selecting a mixture \n",
    "            pi_idx = np.random.choice(range(self.n_gaussians), p=pis[-1, 0, :].detach().cpu().numpy())\n",
    "            \n",
    "            # taking last parameters from sequence corresponding to chosen gaussian\n",
    "            mu1 = mu1s[-1, :, pi_idx].item()\n",
    "            mu2 = mu2s[-1, :, pi_idx].item()\n",
    "            sigma1 = sigma1s[-1, :, pi_idx].item()\n",
    "            sigma2 = sigma2s[-1, :, pi_idx].item()\n",
    "            rho = rhos[-1, :, pi_idx].item()\n",
    "            \n",
    "            prediction = self.generate_sample(mu1, mu2, sigma1, sigma2, rho)\n",
    "            eos = torch.distributions.bernoulli.Bernoulli(torch.tensor([es[-1, :].item()])).sample()\n",
    "            \n",
    "            sample = torch.zeros_like(x0) # torch.Size([1, 1, 3])\n",
    "            sample[0, 0, 0] = prediction[0, 0]\n",
    "            sample[0, 0, 1] = prediction[0, 1]\n",
    "            sample[0, 0, 2] = eos\n",
    "            \n",
    "            sequence = torch.cat((sequence, sample), 0) # torch.Size([sequence_length, 1, 3])\n",
    "            \n",
    "            f.value += 1\n",
    "        \n",
    "        return sequence.squeeze(1).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing density probability\n",
    "\n",
    "It's time to implement the probability density of our next point given our output vector (the Gaussian mixtures parameters). In the paper, this is given by equations 23-25. This will be useful when computing the loss function. \n",
    "\n",
    "![eq23-25](./pictures/eq23-25.png)\n",
    "\n",
    "I chose to exclude the Bernouilli part for now. It will be computed in the loss function.\n",
    "\n",
    "`gaussianMixture(y, pis, mu1s, mu2s, sigma1s, sigma2s, rhos)` :\n",
    "\n",
    "Remember the forward function of our model. gaussianMixture(...) takes for parameters its outputs. As such, it computes the results of equation 23 of the whole sequence over the different batches. A note on parameter y. It is basically the same tensor as x but shifted one time step. Think of it as $x_{t+1}$ in equation 23. It allows the last point of a sequence to still be learned correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussianMixture(y, pis, mu1s, mu2s, sigma1s, sigma2s, rhos):\n",
    "    n_mixtures = pis.size(2)\n",
    "    \n",
    "    # Takes x1 and repeats it over the number of gaussian mixtures\n",
    "    x1 = y[:,:, 0].repeat(n_mixtures, 1, 1).permute(1, 2, 0) \n",
    "    # print(\"x1 shape \", x1.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "    \n",
    "    # first term of Z (eq 25)\n",
    "    x1norm = ((x1 - mu1s) ** 2) / (sigma1s ** 2 )\n",
    "    # print(\"x1norm shape \", x1.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "    \n",
    "    x2 = y[:,:, 1].repeat(n_mixtures, 1, 1).permute(1, 2, 0)  \n",
    "    # print(\"x2 shape \", x2.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "    \n",
    "    # second term of Z (eq 25)\n",
    "    x2norm = ((x2 - mu2s) ** 2) / (sigma2s ** 2 )\n",
    "    # print(\"x2norm shape \", x2.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "    \n",
    "    # third term of Z (eq 25)\n",
    "    coxnorm = 2 * rhos * (x1 - mu1s) * (x2 - mu2s) / (sigma1s * sigma2s) \n",
    "    \n",
    "    # Computing Z (eq 25)\n",
    "    Z = x1norm + x2norm - coxnorm\n",
    "    \n",
    "    # Gaussian bivariate (eq 24)\n",
    "    N = torch.exp(-Z / (2 * (1 - rhos ** 2))) / (2 * np.pi * sigma1s * sigma2s * (1 - rhos ** 2) ** 0.5) \n",
    "    # print(\"N shape \", N.shape) # -> torch.Size([sequence_length, batch, n_gaussians]) \n",
    "    \n",
    "    # Pr is the result of eq 23 without the eos part\n",
    "    Pr = pis * N \n",
    "    # print(\"Pr shape \", Pr.shape) # -> torch.Size([sequence_length, batch, n_gaussians])   \n",
    "    Pr = torch.sum(Pr, dim=2) \n",
    "    # print(\"Pr shape \", Pr.shape) # -> torch.Size([sequence_length, batch])   \n",
    "    \n",
    "    if use_cuda:\n",
    "        Pr = Pr.cuda()\n",
    "    \n",
    "    return Pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing loss fn\n",
    "\n",
    "The goal is to maximize the likelihood of our estimated bivariate normal distributions and Bernoulli distribution. Think about it this way. We generate parameters for our distributions but we want them to fit as best as possible to our data. Each training step's goal is to converge toward the best parameters for our data. [Click here to read more about likelihood function](https://en.wikipedia.org/wiki/Likelihood_function).\n",
    "\n",
    "In the paper, the loss is given by equation 26 :\n",
    "\n",
    "![eq26](./pictures/eq26.png)\n",
    "\n",
    "We previously calculated the first element of the equation in gaussianMixture(...). What's left is to add the Bernoulli loss (second part of our equation). The loss of each time step is summed up and averaged over the batches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(Pr, y, es):\n",
    "    loss1 = - torch.log(Pr + eps) # -> torch.Size([sequence_length, batch])    \n",
    "    bernouilli = torch.zeros_like(es) # -> torch.Size([sequence_length, batch])\n",
    "    \n",
    "    bernouilli = y[:, :, 2] * es + (1 - y[:, :, 2]) * (1 - es)\n",
    "    \n",
    "    loss2 = - torch.log(bernouilli + eps)\n",
    "    loss = loss1 + loss2 \n",
    "    # print(\"loss shape\", loss.shape) # -> torch.Size([sequence_length, batch])  \n",
    "    loss = torch.sum(loss, 0) \n",
    "    # print(\"loss shape\", loss.shape) # -> torch.Size([batch]) \n",
    "    \n",
    "    return torch.mean(loss);\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The hardest part is behind us ! All that's left is to train our model. I used an Adam optimizer with a learning rate of 0.005. I haven't fiddled around too much with it as it already yields good results with my other model (Handwriting prediction - Model 2). The gradients are clipped inside [-gradient_threshold, gradient_treshold] to avoid exploding gradient. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, epochs = 5, generate = True):\n",
    "    data_loader = DataLoader(n_batch, sequence_length, 20, U_items=U_items) # 20 = datascale\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    \n",
    "    # A sequence the model is going to try to write as it learns\n",
    "    c0 = np.float32(one_hot(\"writing is hard!\"))\n",
    "    c0 = torch.from_numpy(c0) \n",
    "    c0 = torch.unsqueeze(c0, 0) # torch.Size(n_batch, U_items, len(alphabet))\n",
    "    start = time.time()\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        c0 = c0.cuda()\n",
    "        \n",
    "    # Arrays to plot loss over time\n",
    "    time_batch = []\n",
    "    time_epoch = [0]\n",
    "    loss_batch = []\n",
    "    loss_epoch = []\n",
    "    \n",
    "    # Loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        data_loader.reset_batch_pointer()\n",
    "        \n",
    "        # Loop over batches\n",
    "        for batch in range(data_loader.num_batches):\n",
    "            # Loading a batch (x : stroke sequences, y : same as x but shifted 1 timestep, c : one-hot encoded character sequence ofx)\n",
    "            x, y, s, c = data_loader.next_batch()\n",
    "            x = np.float32(np.array(x)) # -> (n_batch, sequence_length, 3)\n",
    "            y = np.float32(np.array(y)) # -> (n_batch, sequence_length, 3)\n",
    "            c = np.float32(np.array(c))\n",
    "\n",
    "            x = torch.from_numpy(x).permute(1, 0, 2) # torch.Size([sequence_length, n_batch, 3])\n",
    "            y = torch.from_numpy(y).permute(1, 0, 2) # torch.Size([sequence_length, n_batch, 3])\n",
    "            c = torch.from_numpy(c) # torch.Size(n_batch, U_items, len(alphabet))\n",
    "            \n",
    "            if use_cuda:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                c = c.cuda()\n",
    "            \n",
    "            # Forward pass\n",
    "            es, pis, mu1s, mu2s, sigma1s, sigma2s, rhos = model.forward(x, c)\n",
    "            \n",
    "            # Calculate probability density and loss\n",
    "            Pr = gaussianMixture(y, pis, mu1s, mu2s, sigma1s, sigma2s, rhos)\n",
    "            loss = loss_fn(Pr,y, es)\n",
    "            \n",
    "            # Back propagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient cliping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_threshold)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Useful infos over training\n",
    "            if batch % 10 == 0:\n",
    "                print(\"Epoch : \", epoch, \" - step \", batch, \"/\", data_loader.num_batches, \" - loss \", loss.item(), \" in \", time.time() - start)\n",
    "                start = time.time()\n",
    "                \n",
    "                # Plot heatmaps every 100 batch\n",
    "                if batch % 100 == 0:\n",
    "                    print(s[0])\n",
    "                    plot_heatmaps(model.Phis.transpose(0, 1).detach().numpy(), model.Ws.transpose(0, 1).detach().numpy())\n",
    "                    \n",
    "                # Generate a sequence every 500 batch        \n",
    "                if generate and batch % 500 == 0 and batch > 0:\n",
    "                    x0 = torch.Tensor([0,0,1]).view(1,1,3)\n",
    "\n",
    "                    if use_cuda:\n",
    "                        x0 = x0.cuda()\n",
    "\n",
    "                    sequence = model.generate_sequence(x0, c0)\n",
    "                    print(sequence.shape)\n",
    "                    draw_strokes_random_color(sequence, factor=0.5)\n",
    "                    \n",
    "            # Save loss per batch\n",
    "            time_batch.append(epoch + batch / data_loader.num_batches)\n",
    "            loss_batch.append(loss.item())\n",
    "        \n",
    "        # Save loss per epoch\n",
    "        time_epoch.append(epoch + 1)\n",
    "        loss_epoch.append(sum(loss_batch[epoch * data_loader.num_batches : (epoch + 1)*data_loader.num_batches-1]) / data_loader.num_batches)\n",
    "        \n",
    "        # Save model after each epoch\n",
    "        torch.save(model.state_dict(), \"./models/synthesis.py\")\n",
    "        \n",
    "    # Plot loss \n",
    "    plt.plot(time_batch, loss_batch)\n",
    "    plt.plot(time_epoch, [loss_batch[0]] + loss_epoch, color=\"orange\", linewidth=5)\n",
    "    plt.xlabel(\"Epoch\", fontsize=15)\n",
    "    plt.ylabel(\"Loss\", fontsize=15)\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "    return model\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0.176\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = HandwritingSynthesisModel(hidden_size, n_gaussians, Kmixtures, dropout)\n",
    "\n",
    "model = train_network(model, epochs = 5, generate = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test cell\n",
    "\n",
    "```Python\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "modelTest = HandwritingSynthesisModel(hidden_size, n_gaussians, Kmixtures, dropout)\n",
    "\n",
    "data_loader = DataLoader(n_batch, sequence_length, 20, U_items=U_items) # 20 = datascale\n",
    "x, y, s, c = data_loader.next_batch()\n",
    "x = np.float32(np.array(x)) # -> (n_batch, sequence_length, 3)\n",
    "y = np.float32(np.array(y)) # -> (n_batch, sequence_length, 3)\n",
    "c = np.float32(np.array(c))\n",
    "\n",
    "x = torch.from_numpy(x).permute(1, 0, 2) # torch.Size([sequence_length, n_batch, 3])\n",
    "y = torch.from_numpy(y).permute(1, 0, 2) # torch.Size([sequence_length, n_batch, 3])\n",
    "c = torch.from_numpy(c) # torch.Size(n_batch, U_items, len(alphabet))\n",
    "\n",
    "if use_cuda:\n",
    "    modelTest = modelTest.cuda()\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    c = c.cuda()\n",
    "\n",
    "es, pis, mu1s, mu2s, sigma1s, sigma2s, rhos, Phis, Ws = modelTest.forward(x, c)\n",
    "\n",
    "\n",
    "Pr = gaussianMixture(y, pis, mu1s, mu2s, sigma1s, sigma2s, rhos)\n",
    "loss = loss_fn(Pr,y, es)\n",
    "\n",
    "test = one_hot(\"Will it ever work ?\")\n",
    "\n",
    "x0 = torch.Tensor([0,0,1]).view(1,1,3)\n",
    "c0 = np.float32(one_hot(\"Laurent estun PD\"))\n",
    "c0 = torch.from_numpy(c0) \n",
    "c0 = torch.unsqueeze(c0, 0) # torch.Size(n_batch, U_items, len(alphabet))\n",
    "\n",
    "if use_cuda:\n",
    "    x0 = x0.cuda()\n",
    "\n",
    "sequence = model.generate_sequence(x0, c0)\n",
    "print(sequence.shape)\n",
    "draw_strokes_random_color(sequence, factor=0.5)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
