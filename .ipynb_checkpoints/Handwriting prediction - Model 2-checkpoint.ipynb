{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwriting prediction - Model 2\n",
    "\n",
    "This notebook is a personal attempt at coding Alex Graves RNN to predict handwriting (section 4). The paper can be found [here](https://arxiv.org/abs/1308.0850). It differs from model 1 in structure as it incorporates skip connections. This is an intermediate step before implementing section 5 of the paper. Arguably, it works much better and gives some good results.\n",
    "\n",
    "The goal of this notebook is to implement a network in a straightforward manner. As such, code readability is a priority over performance. The implemented network consists of layers of LSTM followed by a Gaussian mixtures layer. Handwriting is highly variable. It makes more sense to generate a probability density function at each time step for the next stroke to capture that essence.\n",
    "\n",
    "The network appears to be working and generates sequences from a starting point that look like handwriting. It is interesting to note that when generating a sequence, the network chooses a style at random and sticks with it.\n",
    "\n",
    "![example of sample](./pictures/sampleModel2_12.png)\n",
    "![example of sample](./pictures/sampleModel2_10.png)\n",
    "![example of sample](./pictures/sampleModel2_6.png)\n",
    "\n",
    "The network is tweakable in sequence length, number of mixtures and dropout probability.\n",
    "\n",
    "The notebook is divided into data treatment (I used [Greydanus's code](https://nbviewer.jupyter.org/github/greydanus/scribe/blob/master/dataloader.ipynb) for that as that part is boring, a variation from hardmaru's code), network class, loss function and training. \n",
    "\n",
    "The dataset comes from [IAM On-Line Handwriting Database](http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database). Download data/lineStrokes-all.tar.gz after signing up ! The path should be ./data/lineStrokes if you want to use this notebook.\n",
    "\n",
    "Enjoy :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "import svgwrite\n",
    "from IPython.display import SVG, display\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "use_cuda = False\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "\n",
    "\n",
    "# Network configuration\n",
    "n_batch = 20\n",
    "sequence_length = 300\n",
    "hidden_size = 256\n",
    "U_items = int(sequence_length/25)\n",
    "n_layers = 3\n",
    "n_gaussians = 20\n",
    "Kmixtures = 10\n",
    "gradient_threshold = 10\n",
    "dropout = 0.2\n",
    "\n",
    "# Small number to avoid log(0) issue\n",
    "eps = float(np.finfo(np.float32).eps)\n",
    "\n",
    "# The network could use the extra space :)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "This code comes from [Greydanus](https://nbviewer.jupyter.org/github/greydanus/scribe/blob/master/dataloader.ipynb). Big thanks to his author !\n",
    "\n",
    "That part is not that fun. Dataloader is a class that parses all the .xml files. It creates a pickle file for future use. It creates a training set containing sequences x, y (same as x but shifted one timestep) and c (one-hot encoding of the sequence) in batches depending on the sequence length. Function `next_batch()` neatly returns a batch. Use `reset_batch_pointer()` to reset the current batch. See the training function for a proper use of that wonderful code. \n",
    "\n",
    "In this notebook, we won't use the hot-one vectors as it is used to implement the attention mechanism of section 5 of the paper.\n",
    "\n",
    "Some examples of training data :\n",
    "\n",
    "![batch2](./pictures/batch_model2.png)\n",
    "\n",
    "And some example code to load the data :\n",
    "\n",
    "```python\n",
    "x, y, s, c = data_loader.next_batch()\n",
    "print (data_loader.pointer)\n",
    "for i in range(n_batch):\n",
    "    r = x[i]\n",
    "    strokes = r.copy()\n",
    "    strokes[:,:-1] = np.cumsum(r[:,:-1], axis=0)\n",
    "    line_plot(strokes, s[i][:U_items])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounds(data, factor):\n",
    "    min_x = 0\n",
    "    max_x = 0\n",
    "    min_y = 0\n",
    "    max_y = 0\n",
    "\n",
    "    abs_x = 0\n",
    "    abs_y = 0\n",
    "    for i in range(len(data)):\n",
    "        x = float(data[i, 0]) / factor\n",
    "        y = float(data[i, 1]) / factor\n",
    "        abs_x += x\n",
    "        abs_y += y\n",
    "        min_x = min(min_x, abs_x)\n",
    "        min_y = min(min_y, abs_y)\n",
    "        max_x = max(max_x, abs_x)\n",
    "        max_y = max(max_y, abs_y)\n",
    "\n",
    "    return (min_x, max_x, min_y, max_y)\n",
    "\n",
    "# old version, where each path is entire stroke (smaller svg size, but\n",
    "# have to keep same color)\n",
    "\n",
    "\n",
    "def draw_strokes(data, factor=10, svg_filename='sample.svg'):\n",
    "    min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
    "    dims = (50 + max_x - min_x, 50 + max_y - min_y)\n",
    "\n",
    "    dwg = svgwrite.Drawing(svg_filename, size=dims)\n",
    "    dwg.add(dwg.rect(insert=(0, 0), size=dims, fill='white'))\n",
    "\n",
    "    lift_pen = 1\n",
    "\n",
    "    abs_x = 25 - min_x\n",
    "    abs_y = 25 - min_y\n",
    "    p = \"M%s,%s \" % (abs_x, abs_y)\n",
    "\n",
    "    command = \"m\"\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if (lift_pen == 1):\n",
    "            command = \"m\"\n",
    "        elif (command != \"l\"):\n",
    "            command = \"l\"\n",
    "        else:\n",
    "            command = \"\"\n",
    "        x = float(data[i, 0]) / factor\n",
    "        y = float(data[i, 1]) / factor\n",
    "        lift_pen = data[i, 2]\n",
    "        p += command + str(x) + \",\" + str(y) + \" \"\n",
    "\n",
    "    the_color = \"black\"\n",
    "    stroke_width = 1\n",
    "\n",
    "    dwg.add(dwg.path(p).stroke(the_color, stroke_width).fill(\"none\"))\n",
    "\n",
    "    dwg.save()\n",
    "    display(SVG(dwg.tostring()))\n",
    "\n",
    "\n",
    "def draw_strokes_eos_weighted(\n",
    "        stroke,\n",
    "        param,\n",
    "        factor=10,\n",
    "        svg_filename='sample_eos.svg'):\n",
    "    c_data_eos = np.zeros((len(stroke), 3))\n",
    "    for i in range(len(param)):\n",
    "        # make color gray scale, darker = more likely to eos\n",
    "        c_data_eos[i, :] = (1 - param[i][6][0]) * 225\n",
    "    draw_strokes_custom_color(\n",
    "        stroke,\n",
    "        factor=factor,\n",
    "        svg_filename=svg_filename,\n",
    "        color_data=c_data_eos,\n",
    "        stroke_width=3)\n",
    "\n",
    "\n",
    "def draw_strokes_random_color(\n",
    "        stroke,\n",
    "        factor=10,\n",
    "        svg_filename='sample_random_color.svg',\n",
    "        per_stroke_mode=True):\n",
    "    c_data = np.array(np.random.rand(len(stroke), 3) * 240, dtype=np.uint8)\n",
    "    if per_stroke_mode:\n",
    "        switch_color = False\n",
    "        for i in range(len(stroke)):\n",
    "            if switch_color == False and i > 0:\n",
    "                c_data[i] = c_data[i - 1]\n",
    "            if stroke[i, 2] < 1:  # same strike\n",
    "                switch_color = False\n",
    "            else:\n",
    "                switch_color = True\n",
    "    draw_strokes_custom_color(\n",
    "        stroke,\n",
    "        factor=factor,\n",
    "        svg_filename=svg_filename,\n",
    "        color_data=c_data,\n",
    "        stroke_width=2)\n",
    "\n",
    "\n",
    "def draw_strokes_custom_color(\n",
    "        data,\n",
    "        factor=10,\n",
    "        svg_filename='test.svg',\n",
    "        color_data=None,\n",
    "        stroke_width=1):\n",
    "    min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
    "    dims = (50 + max_x - min_x, 50 + max_y - min_y)\n",
    "\n",
    "    dwg = svgwrite.Drawing(svg_filename, size=dims)\n",
    "    dwg.add(dwg.rect(insert=(0, 0), size=dims, fill='white'))\n",
    "\n",
    "    lift_pen = 1\n",
    "    abs_x = 25 - min_x\n",
    "    abs_y = 25 - min_y\n",
    "\n",
    "    for i in range(len(data)):\n",
    "\n",
    "        x = float(data[i, 0]) / factor\n",
    "        y = float(data[i, 1]) / factor\n",
    "\n",
    "        prev_x = abs_x\n",
    "        prev_y = abs_y\n",
    "\n",
    "        abs_x += x\n",
    "        abs_y += y\n",
    "\n",
    "        if (lift_pen == 1):\n",
    "            p = \"M \" + str(abs_x) + \",\" + str(abs_y) + \" \"\n",
    "        else:\n",
    "            p = \"M +\" + str(prev_x) + \",\" + str(prev_y) + \\\n",
    "                \" L \" + str(abs_x) + \",\" + str(abs_y) + \" \"\n",
    "\n",
    "        lift_pen = data[i, 2]\n",
    "\n",
    "        the_color = \"black\"\n",
    "\n",
    "        if (color_data is not None):\n",
    "            the_color = \"rgb(\" + str(int(color_data[i, 0])) + \",\" + str(\n",
    "                int(color_data[i, 1])) + \",\" + str(int(color_data[i, 2])) + \")\"\n",
    "\n",
    "        dwg.add(dwg.path(p).stroke(the_color, stroke_width).fill(the_color))\n",
    "    dwg.save()\n",
    "    display(SVG(dwg.tostring()))\n",
    "\n",
    "        \n",
    "class DataLoader():\n",
    "    def __init__(self, batch_size=50, tsteps=300, scale_factor = 10, U_items=10, limit = 500, alphabet=\"default\"):\n",
    "        self.data_dir = \"./data\"\n",
    "        self.alphabet = alphabet\n",
    "        self.batch_size = batch_size\n",
    "        self.tsteps = tsteps\n",
    "        self.scale_factor = scale_factor # divide data by this factor\n",
    "        self.limit = limit # removes large noisy gaps in the data\n",
    "        self.U_items = U_items\n",
    "\n",
    "        data_file = os.path.join(self.data_dir, \"strokes_training_data_generation.cpkl\")\n",
    "        stroke_dir = self.data_dir+\"/lineStrokes\"\n",
    "        ascii_dir = self.data_dir+\"/ascii\"\n",
    "\n",
    "        if not (os.path.exists(data_file)) :\n",
    "            print (\"creating training data cpkl file from raw source\")\n",
    "            self.preprocess(stroke_dir, ascii_dir, data_file)\n",
    "\n",
    "        self.load_preprocessed(data_file)\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, stroke_dir, ascii_dir, data_file):\n",
    "        # create data file from raw xml files from iam handwriting source.\n",
    "        print (\"Parsing dataset...\")\n",
    "        \n",
    "        # build the list of xml files\n",
    "        filelist = []\n",
    "        # Set the directory you want to start from\n",
    "        rootDir = stroke_dir\n",
    "        for dirName, subdirList, fileList in os.walk(rootDir):\n",
    "#             print('Found directory: %s' % dirName)\n",
    "            for fname in fileList:\n",
    "#                 print('\\t%s' % fname)\n",
    "                filelist.append(dirName+\"/\"+fname)\n",
    "\n",
    "        # function to read each individual xml file\n",
    "        def getStrokes(filename):\n",
    "            tree = ET.parse(filename)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            result = []\n",
    "\n",
    "            x_offset = 1e20\n",
    "            y_offset = 1e20\n",
    "            y_height = 0\n",
    "            for i in range(1, 4):\n",
    "                x_offset = min(x_offset, float(root[0][i].attrib['x']))\n",
    "                y_offset = min(y_offset, float(root[0][i].attrib['y']))\n",
    "                y_height = max(y_height, float(root[0][i].attrib['y']))\n",
    "            y_height -= y_offset\n",
    "            x_offset -= 100\n",
    "            y_offset -= 100\n",
    "\n",
    "            for stroke in root[1].findall('Stroke'):\n",
    "                points = []\n",
    "                for point in stroke.findall('Point'):\n",
    "                    points.append([float(point.attrib['x'])-x_offset,float(point.attrib['y'])-y_offset])\n",
    "                result.append(points)\n",
    "            return result\n",
    "        \n",
    "        # function to read each individual xml file\n",
    "        def getAscii(filename, line_number):\n",
    "            with open(filename, \"r\") as f:\n",
    "                s = f.read()\n",
    "            s = s[s.find(\"CSR\"):]\n",
    "            if len(s.split(\"\\n\")) > line_number+2:\n",
    "                s = s.split(\"\\n\")[line_number+2]\n",
    "                return s\n",
    "            else:\n",
    "                return \"\"\n",
    "                \n",
    "        # converts a list of arrays into a 2d numpy int16 array\n",
    "        def convert_stroke_to_array(stroke):\n",
    "            n_point = 0\n",
    "            for i in range(len(stroke)):\n",
    "                n_point += len(stroke[i])\n",
    "            stroke_data = np.zeros((n_point, 3), dtype=np.int16)\n",
    "\n",
    "            prev_x = 0\n",
    "            prev_y = 0\n",
    "            counter = 0\n",
    "\n",
    "            for j in range(len(stroke)):\n",
    "                for k in range(len(stroke[j])):\n",
    "                    stroke_data[counter, 0] = int(stroke[j][k][0]) - prev_x\n",
    "                    stroke_data[counter, 1] = int(stroke[j][k][1]) - prev_y\n",
    "                    prev_x = int(stroke[j][k][0])\n",
    "                    prev_y = int(stroke[j][k][1])\n",
    "                    stroke_data[counter, 2] = 0\n",
    "                    if (k == (len(stroke[j])-1)): # end of stroke\n",
    "                        stroke_data[counter, 2] = 1\n",
    "                    counter += 1\n",
    "            return stroke_data\n",
    "\n",
    "        # build stroke database of every xml file inside iam database\n",
    "        strokes = []\n",
    "        asciis = []\n",
    "        for i in range(len(filelist)):\n",
    "            if (filelist[i][-3:] == 'xml'):\n",
    "                stroke_file = filelist[i]\n",
    "#                 print 'processing '+stroke_file\n",
    "                stroke = convert_stroke_to_array(getStrokes(stroke_file))\n",
    "                \n",
    "                ascii_file = stroke_file.replace(\"lineStrokes\",\"ascii\")[:-7] + \".txt\"\n",
    "                line_number = stroke_file[-6:-4]\n",
    "                line_number = int(line_number) - 1\n",
    "                ascii = getAscii(ascii_file, line_number)\n",
    "                if len(ascii) > 10:\n",
    "                    strokes.append(stroke)\n",
    "                    asciis.append(ascii)\n",
    "                else:\n",
    "                    print (\"======>>>> Line length was too short. Line was: \" + ascii)\n",
    "                \n",
    "        assert(len(strokes)==len(asciis)), \"There should be a 1:1 correspondence between stroke data and ascii labels.\"\n",
    "        f = open(data_file,\"wb\")\n",
    "        pickle.dump([strokes,asciis], f, protocol=2)\n",
    "        f.close()\n",
    "        print (\"Finished parsing dataset. Saved {} lines\".format(len(strokes)))\n",
    "\n",
    "\n",
    "    def load_preprocessed(self, data_file):\n",
    "        f = open(data_file,\"rb\")\n",
    "        [self.raw_stroke_data, self.raw_ascii_data] = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "        # goes thru the list, and only keeps the text entries that have more than tsteps points\n",
    "        self.stroke_data = []\n",
    "        self.ascii_data = []\n",
    "        counter = 0\n",
    "\n",
    "        for i in range(len(self.raw_stroke_data)):\n",
    "            data = self.raw_stroke_data[i]\n",
    "            if len(data) > (self.tsteps+2):\n",
    "                # removes large gaps from the data\n",
    "                data = np.minimum(data, self.limit)\n",
    "                data = np.maximum(data, -self.limit)\n",
    "                data = np.array(data,dtype=np.float32)\n",
    "                data[:,0:2] /= self.scale_factor\n",
    "                \n",
    "                self.stroke_data.append(data)\n",
    "                self.ascii_data.append(self.raw_ascii_data[i])\n",
    "\n",
    "        # minus 1, since we want the ydata to be a shifted version of x data\n",
    "        self.num_batches = int(len(self.stroke_data) / self.batch_size)\n",
    "        print (\"Loaded dataset:\")\n",
    "        print (\"   -> {} individual data points\".format(len(self.stroke_data)))\n",
    "        print (\"   -> {} batches\".format(self.num_batches))\n",
    "\n",
    "    def next_batch(self):\n",
    "        # returns a randomised, tsteps sized portion of the training data\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        ascii_list = []\n",
    "        for i in range(self.batch_size):\n",
    "            data = self.stroke_data[self.idx_perm[self.pointer]]\n",
    "            x_batch.append(np.copy(data[:self.tsteps]))\n",
    "            y_batch.append(np.copy(data[1:self.tsteps+1]))\n",
    "            ascii_list.append(self.ascii_data[self.idx_perm[self.pointer]])\n",
    "            self.tick_batch_pointer()\n",
    "        one_hots = [self.one_hot(s) for s in ascii_list]\n",
    "        return x_batch, y_batch, ascii_list, one_hots\n",
    "    \n",
    "    def one_hot(self, s):\n",
    "        #index position 0 means \"unknown\"\n",
    "        if self.alphabet is \"default\":\n",
    "            alphabet = \" abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890\"\n",
    "        seq = [alphabet.find(char) + 1 for char in s]\n",
    "        if len(seq) >= self.U_items:\n",
    "            seq = seq[:self.U_items]\n",
    "        else:\n",
    "            seq = seq + [0]*(self.U_items - len(seq))\n",
    "        one_hot = np.zeros((self.U_items,len(alphabet)+1))\n",
    "        one_hot[np.arange(self.U_items),seq] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def tick_batch_pointer(self):\n",
    "        self.pointer += 1\n",
    "        if (self.pointer >= len(self.stroke_data)):\n",
    "            self.reset_batch_pointer()\n",
    "    def reset_batch_pointer(self):\n",
    "        self.idx_perm = np.random.permutation(len(self.stroke_data))\n",
    "        self.pointer = 0\n",
    "        print (\"pointer reset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "This is where the fun begins. The network consists of LSTM cells stacked on top of each other and followed by a Gaussian mixture layer. This network includes skip connections like the paper. \n",
    "\n",
    "`__init__(self, hidden_size = 256, n_gaussians = 10, dropout = 0.2)` :\n",
    "This is the constructor. It takes the different parameters to create the different blocks of the model.\n",
    "- hidden_size is the size of the output of each LSTM cell\n",
    "- n_gaussians is the number of mixtures\n",
    "- dropout is the dropout probability. It gives the probability to skip a cell during forward propagation. It's not implemented actually\n",
    "\n",
    "The Gaussian mixtures are created using a dense layer. It takes the output of the last LSTM layer. Say the hidden size is 256 and you want 10 mixtures, this allows to scale your vector to the desired size. This gives ŷ of equation 17 of the paper.\n",
    "![eq17](./pictures/eq17.png)\n",
    "ŷ is then broken down into the different parameters of the mixture. \n",
    "- ê is the probability of the end of a stroke given by a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution)\n",
    "- w (or $\\Pi$ ) is the weight of each Normal distribution\n",
    "- $\\mu, \\sigma, \\rho$ are the mean, standard deviation and correlation factor of each [bivariate Normal Distribution](http://mathworld.wolfram.com/BivariateNormalDistribution.html)\n",
    "The constructor juste lays out the blocks but does not create relations between them. That's the job of the forward function.\n",
    "\n",
    "\n",
    "`forward(self, x, hidden = None)` :\n",
    "This is the forward propagation. It takes x, a batch of dimensions [sequence_size, batch_size, 3]. The 3 corresponds to x and y offset of a stroke and eos (= 1 when reaching an end of stroke (when the pen is raised)). Note that the forward function is also used to generate random sequences.\n",
    "\n",
    "The first step is to compute the LSTM cells block. This is straightfoward in PyTorch. Since I created LSTM cells I need a for loop over the whole sequence.\n",
    "\n",
    "Then it's just a matter of computing 18 - 22 of the paper.\n",
    "![eq18-22](./pictures/eq18-22.png)\n",
    "\n",
    "\n",
    "`generate_sequence(self, x0, sequence_length = 100)` :\n",
    "This is where I clearly sacrifice performance for readability. The goal of this function is to return a sequence based on either a single point or begining of sequence x0. In pseudo-code :\n",
    "- Calculte the mixture parameters of sequence x0\n",
    "- Pick a random mixture based on the weights (pi_idx)\n",
    "- Take a random point from the chosen bivariate normal distribution\n",
    "- Add it at the end of the sequence (concatenate it)\n",
    "- Repeat\n",
    "\n",
    "\n",
    "This clearly is bad practise as I have to rerun the forward prop on the entire sequence each time. And the sequence gets longer and longer which takes more time to compute at each new point generated. However this holds in just a few lines and keeps the forward function cleaner.\n",
    "\n",
    "\n",
    "`generate_sample(self, mu1, mu2, sigma1, sigma2, rho)` :\n",
    "Returns random coordinates based on a bivariate normal distribution given by the function parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandwritingGenerationModel2(nn.Module):\n",
    "    def __init__(self, hidden_size = 256, n_gaussians = 20, dropout = 0.2):\n",
    "        super(HandwritingGenerationModel2, self).__init__()\n",
    "        \n",
    "        self.n_gaussians = n_gaussians\n",
    "        \n",
    "        self.hidden_size1 = hidden_size\n",
    "        self.hidden_size2 = hidden_size\n",
    "        self.hidden_size3 = hidden_size\n",
    "        \n",
    "        # input_size1 includes x, y, eos \n",
    "        self.input_size1 = 3\n",
    "        \n",
    "        # input_size2 includes x, y, eos and hidden_size1\n",
    "        self.input_size2 = 3 + self.hidden_size1\n",
    "        \n",
    "        # input_size3 includes x, y, eos and hidden_size2\n",
    "        self.input_size3 = 3 + self.hidden_size2\n",
    "        \n",
    "        # Creating the LSTM cells\n",
    "        self.lstm1 = nn.LSTMCell(input_size= self.input_size1 , hidden_size = self.hidden_size1)\n",
    "        self.lstm2 = nn.LSTMCell(input_size= self.input_size2 , hidden_size = self.hidden_size2)\n",
    "        self.lstm3 = nn.LSTMCell(input_size= self.input_size3 , hidden_size = self.hidden_size3)\n",
    "        \n",
    "        # For gaussian mixtures\n",
    "        self.z_e = nn.Linear(hidden_size, 1)\n",
    "        self.z_pi = nn.Linear(hidden_size, n_gaussians)\n",
    "        self.z_mu1 = nn.Linear(hidden_size, n_gaussians)\n",
    "        self.z_mu2 = nn.Linear(hidden_size, n_gaussians)\n",
    "        self.z_sigma1 = nn.Linear(hidden_size, n_gaussians)\n",
    "        self.z_sigma2 = nn.Linear(hidden_size, n_gaussians)\n",
    "        self.z_rho = nn.Linear(hidden_size, n_gaussians)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # sequence length\n",
    "        sequence_length = x.shape[0]\n",
    "        \n",
    "        # number of batches\n",
    "        n_batch = x.shape[1]\n",
    "        \n",
    "        # Hidden and cell state for LSTM1\n",
    "        h1_t = torch.zeros(n_batch, self.hidden_size1) # torch.Size([n_batch, hidden_size1])\n",
    "        c1_t = torch.zeros(n_batch, self.hidden_size1) # torch.Size([n_batch, hidden_size1])\n",
    "        \n",
    "        # Hidden and cell state for LSTM2\n",
    "        h2_t = torch.zeros(n_batch, self.hidden_size2) # torch.Size([n_batch, hidden_size2])\n",
    "        c2_t = torch.zeros(n_batch, self.hidden_size2) # torch.Size([n_batch, hidden_size2])\n",
    "        \n",
    "        # Hidden and cell state for LSTM2\n",
    "        h3_t = torch.zeros(n_batch, self.hidden_size3) # torch.Size([n_batch, hidden_size3])\n",
    "        c3_t = torch.zeros(n_batch, self.hidden_size3) # torch.Size([n_batch, hidden_size3])\n",
    "        \n",
    "        # Outputs of LSTM3 over the whole sequence\n",
    "        out = torch.zeros(sequence_length, n_batch, self.hidden_size3)\n",
    "        \n",
    "        if use_cuda:\n",
    "            h1_t = h1_t.cuda()\n",
    "            c1_t = c1_t.cuda()\n",
    "            \n",
    "            h2_t = h2_t.cuda()\n",
    "            c2_t = c2_t.cuda()\n",
    "            \n",
    "            h3_t = h3_t.cuda()\n",
    "            c3_t = c3_t.cuda()\n",
    "            \n",
    "            out = out.cuda()\n",
    "\n",
    "            \n",
    "        for i in range(sequence_length):\n",
    "            # ===== Computing 1st layer =====\n",
    "            h1_t, c1_t = self.lstm1(x[i], (h1_t, c1_t)) # torch.Size([n_batch, hidden_size1])\n",
    "            \n",
    "            \n",
    "            # ===== Computing 2nd layer =====\n",
    "            input_lstm2 = torch.cat((x[i], h1_t), 1) # torch.Size([n_batch, 3 + hidden_size1])\n",
    "            h2_t, c2_t = self.lstm2(input_lstm2, (h2_t, c2_t)) \n",
    "            \n",
    "            \n",
    "            # ===== Computing 3rd layer =====\n",
    "            input_lstm3 = torch.cat((x[i], h2_t), 1) # torch.Size([n_batch, 3 + alphabet_size + hidden_size2])\n",
    "            h3_t, c3_t = self.lstm2(input_lstm2, (h3_t, c3_t))\n",
    "            out[i, :, :] = h3_t\n",
    "            \n",
    "            \n",
    "        # ===== Computing MDN =====\n",
    "        es = self.z_e(out)\n",
    "        # print(\"es shape \", es.shape) # -> torch.Size([sequence_length, batch, 1])\n",
    "        es = 1 / (1 + torch.exp(es))\n",
    "        # print(\"es shape\", es.shape) # -> torch.Size([sequence_length, batch, 1])\n",
    "\n",
    "        pis = self.z_pi(out)\n",
    "        # print(\"pis shape \", pis.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "        pis = torch.softmax(pis, 2)\n",
    "        # print(pis.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "\n",
    "        mu1s = self.z_mu1(out)\n",
    "        mu2s = self.z_mu2(out)\n",
    "        # print(\"mu shape :  \", mu1s.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "\n",
    "        sigma1s = self.z_sigma1(out)\n",
    "        sigma2s = self.z_sigma2(out)\n",
    "        # print(\"sigmas shape \", sigma1s.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "        sigma1s = torch.exp(sigma1s)\n",
    "        sigma2s = torch.exp(sigma2s)\n",
    "        # print(sigma1s.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "\n",
    "        rhos = self.z_rho(out)\n",
    "        rhos = torch.tanh(rhos)\n",
    "        # print(\"rhos shape \", rhos.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "\n",
    "        es = es.squeeze(2) \n",
    "        # print(\"es shape \", es.shape) # -> torch.Size([sequence_length, batch])\n",
    "\n",
    "\n",
    "        return es, pis, mu1s, mu2s, sigma1s, sigma2s, rhos\n",
    "    \n",
    "    \n",
    "    def generate_sample(self, mu1, mu2, sigma1, sigma2, rho):\n",
    "        mean = [mu1, mu2]\n",
    "        cov = [[sigma1 ** 2, rho * sigma1 * sigma2], [rho * sigma1 * sigma2, sigma2 ** 2]]\n",
    "        \n",
    "        x = np.float32(np.random.multivariate_normal(mean, cov, 1))\n",
    "        return torch.from_numpy(x)\n",
    "        \n",
    "        \n",
    "    def generate_sequence(self, x0, sequence_length = 100):\n",
    "        sequence = x0\n",
    "        \n",
    "        # A fun little widget\n",
    "        print(\"Generating sequence ...\")\n",
    "        f = FloatProgress(min=0, max=sequence_length)\n",
    "        display(f)\n",
    "        \n",
    "        for i in range(sequence_length):\n",
    "            es, pis, mu1s, mu2s, sigma1s, sigma2s, rhos = self.forward(sequence)\n",
    "            \n",
    "            # Selecting a mixture \n",
    "            pi_idx = np.random.choice(range(self.n_gaussians), p=pis[-1, 0, :].detach().cpu().numpy())\n",
    "            \n",
    "            # taking last parameters from sequence corresponding to chosen gaussian\n",
    "            mu1 = mu1s[-1, :, pi_idx].item()\n",
    "            mu2 = mu2s[-1, :, pi_idx].item()\n",
    "            sigma1 = sigma1s[-1, :, pi_idx].item()\n",
    "            sigma2 = sigma2s[-1, :, pi_idx].item()\n",
    "            rho = rhos[-1, :, pi_idx].item()\n",
    "            \n",
    "            prediction = self.generate_sample(mu1, mu2, sigma1, sigma2, rho)\n",
    "            eos = torch.distributions.bernoulli.Bernoulli(torch.tensor([es[-1, :].item()])).sample()\n",
    "            \n",
    "            sample = torch.zeros_like(x0) # torch.Size([1, 1, 3])\n",
    "            sample[0, 0, 0] = prediction[0, 0]\n",
    "            sample[0, 0, 1] = prediction[0, 1]\n",
    "            sample[0, 0, 2] = eos\n",
    "            \n",
    "            sequence = torch.cat((sequence, sample), 0) # torch.Size([sequence_length, 1, 3])\n",
    "            \n",
    "            f.value += 1\n",
    "        \n",
    "        return sequence.squeeze(1).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing density probability\n",
    "\n",
    "It's time to implement the probability density of our next point given our output vector (the Gaussian mixtures parameters). In the paper, this is given by equations 23-25. This will be useful when computing the loss function. \n",
    "\n",
    "![eq23-25](./pictures/eq23-25.png)\n",
    "\n",
    "I chose to exclude the Bernouilli part for now. It will be computed in the loss function.\n",
    "\n",
    "`gaussianMixture(y, pis, mu1s, mu2s, sigma1s, sigma2s, rhos)` :\n",
    "\n",
    "Remember the forward function of our model. gaussianMixture(...) takes for parameters its outputs. As such, it computes the results of equation 23 of the whole sequence over the different batches. A note on parameter y. It is basically the same tensor as x but shifted one time step. Think of it as $x_{t+1}$ in equation 23. It allows the last point of a sequence to still be learned correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussianMixture(y, pis, mu1s, mu2s, sigma1s, sigma2s, rhos):\n",
    "    n_mixtures = pis.size(2)\n",
    "    \n",
    "    # Takes x1 and repeats it over the number of gaussian mixtures\n",
    "    x1 = y[:,:, 0].repeat(n_mixtures, 1, 1).permute(1, 2, 0) \n",
    "    # print(\"x1 shape \", x1.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "    \n",
    "    # first term of Z (eq 25)\n",
    "    x1norm = ((x1 - mu1s) ** 2) / (sigma1s ** 2 )\n",
    "    # print(\"x1norm shape \", x1.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "    \n",
    "    x2 = y[:,:, 1].repeat(n_mixtures, 1, 1).permute(1, 2, 0)  \n",
    "    # print(\"x2 shape \", x2.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "    \n",
    "    # second term of Z (eq 25)\n",
    "    x2norm = ((x2 - mu2s) ** 2) / (sigma2s ** 2 )\n",
    "    # print(\"x2norm shape \", x2.shape) # -> torch.Size([sequence_length, batch, n_gaussians])\n",
    "    \n",
    "    # third term of Z (eq 25)\n",
    "    coxnorm = 2 * rhos * (x1 - mu1s) * (x2 - mu2s) / (sigma1s * sigma2s) \n",
    "    \n",
    "    # Computing Z (eq 25)\n",
    "    Z = x1norm + x2norm - coxnorm\n",
    "    \n",
    "    # Gaussian bivariate (eq 24)\n",
    "    N = torch.exp(-Z / (2 * (1 - rhos ** 2))) / (2 * np.pi * sigma1s * sigma2s * (1 - rhos ** 2) ** 0.5) \n",
    "    # print(\"N shape \", N.shape) # -> torch.Size([sequence_length, batch, n_gaussians]) \n",
    "    \n",
    "    # Pr is the result of eq 23 without the eos part\n",
    "    Pr = pis * N \n",
    "    # print(\"Pr shape \", Pr.shape) # -> torch.Size([sequence_length, batch, n_gaussians])   \n",
    "    Pr = torch.sum(Pr, dim=2) \n",
    "    # print(\"Pr shape \", Pr.shape) # -> torch.Size([sequence_length, batch])   \n",
    "    \n",
    "    if use_cuda:\n",
    "        Pr = Pr.cuda()\n",
    "    \n",
    "    return Pr\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing loss fn\n",
    "\n",
    "The goal is to maximize the likelihood of our estimated bivariate normal distributions and Bernoulli distribution. Think about it this way. We generate parameters for our distributions but we want them to fit as best as possible to our data. Each training step's goal is to converge toward the best parameters for our data. [Click here to read more about likelihood function](https://en.wikipedia.org/wiki/Likelihood_function).\n",
    "\n",
    "In the paper, the loss is given by equation 26 :\n",
    "\n",
    "![eq26](./pictures/eq26.png)\n",
    "\n",
    "We previously calculated the first element of the equation in gaussianMixture(...). What's left is to add the Bernoulli loss (second part of our equation). The loss of each time step is summed up and averaged over the batches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(Pr, y, es):\n",
    "    loss1 = - torch.log(Pr + eps) # -> torch.Size([sequence_length, batch])    \n",
    "    bernouilli = torch.zeros_like(es) # -> torch.Size([sequence_length, batch])\n",
    "    \n",
    "    bernouilli = y[:, :, 2] * es + (1 - y[:, :, 2]) * (1 - es)\n",
    "    \n",
    "    loss2 = - torch.log(bernouilli + eps)\n",
    "    loss = loss1 + loss2 \n",
    "    # print(\"loss shape\", loss.shape) # -> torch.Size([sequence_length, batch])  \n",
    "    loss = torch.sum(loss, 0) \n",
    "    # print(\"loss shape\", loss.shape) # -> torch.Size([batch]) \n",
    "    \n",
    "    return torch.mean(loss);\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The hardest part is behind us ! All that's left is to train our model. I used an Adam optimizer with a learning rate of 0.005. I haven't fiddled around too much with it as it already yields good results. The gradients are clipped inside [-gradient_threshold, gradient_treshold] to avoid exploding gradient. A sequence is generated every 100 batches to see how the model is learning. Looks like it works !\n",
    "\n",
    "![sample](./pictures/sampleModel2_8.png)\n",
    "![sample](./pictures/sampleModel2_9.png)\n",
    "![sample](./pictures/sampleModel2_10.png)\n",
    "![sample](./pictures/sampleModel2_11.png)\n",
    "\n",
    "The network is able to pick a style and stick with it. Of course it is unreadable but is is convincing enough.\n",
    "\n",
    "This is what it looks like training\n",
    "![training](./pictures/training_model1.png)\n",
    "\n",
    "And the loss function after 10 epochs. In orange is the average loss per epoch, in blue the loss per batch.\n",
    "![loss plot](./pictures/model2_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, epochs = 5, generate = True):\n",
    "    data_loader = DataLoader(n_batch, sequence_length, 20) # 20 = datascale\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        \n",
    "    # Arrays to plot loss over time\n",
    "    time_batch = []\n",
    "    time_epoch = [0]\n",
    "    loss_batch = []\n",
    "    loss_epoch = []\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # Loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        data_loader.reset_batch_pointer()\n",
    "        \n",
    "        # Loop over batches\n",
    "        for batch in range(data_loader.num_batches):\n",
    "            # Loading a batch\n",
    "            x, y, _, _ = data_loader.next_batch()\n",
    "            x = np.float32(np.array(x)) # -> (n_batch, sequence_length, 3)\n",
    "            y = np.float32(np.array(y)) # -> (n_batch, sequence_length, 3)\n",
    "\n",
    "            x = torch.from_numpy(x).permute(1, 0, 2) # torch.Size([sequence_length, n_batch, 3])\n",
    "            y = torch.from_numpy(y).permute(1, 0, 2) # torch.Size([sequence_length, n_batch, 3])\n",
    "            \n",
    "            if use_cuda:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "            \n",
    "            # Forward pass\n",
    "            es, pis, mu1s, mu2s, sigma1s, sigma2s, rhos = model.forward(x)\n",
    "            \n",
    "            # Calculate probability density and loss\n",
    "            Pr = gaussianMixture(y, pis, mu1s, mu2s, sigma1s, sigma2s, rhos)\n",
    "            loss = loss_fn(Pr,y, es)\n",
    "            \n",
    "            # Back propagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient cliping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_threshold)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Useful infos over training\n",
    "            if batch % 20 == 0:\n",
    "                print(\"Epoch : \", epoch, \" - step \", batch, \"/\", data_loader.num_batches, \" - loss \", loss.item(), \" in \", time.time() - start)\n",
    "                start = time.time()\n",
    "            \n",
    "                if generate and batch % 500 == 0:\n",
    "                    x0 = torch.Tensor([0,0,1]).view(1,1,3)\n",
    "\n",
    "                    if use_cuda:\n",
    "                        x0 = x0.cuda()\n",
    "\n",
    "                    sequence = model.generate_sequence(x0, sequence_length = 500)\n",
    "                    draw_strokes_random_color(sequence, factor=0.5)\n",
    "                    \n",
    "                    \n",
    "            # Save loss per batch\n",
    "            time_batch.append(epoch + batch / data_loader.num_batches)\n",
    "            loss_batch.append(loss.item())\n",
    "        \n",
    "        # Save loss per epoch\n",
    "        time_epoch.append(epoch + 1)\n",
    "        loss_epoch.append(sum(loss_batch[epoch * data_loader.num_batches : (epoch + 1)*data_loader.num_batches-1]) / data_loader.num_batches)\n",
    "        \n",
    "        # Save model after each epoch\n",
    "        torch.save(model.state_dict(), \"./models/prediction_model2.py\")\n",
    "        \n",
    "    # Plot loss \n",
    "    plt.plot(time_batch, loss_batch)\n",
    "    plt.plot(time_epoch, [loss_batch[0]] + loss_epoch, color=\"orange\", linewidth=5)\n",
    "    plt.xlabel(\"Epoch\", fontsize=15)\n",
    "    plt.ylabel(\"Loss\", fontsize=15)\n",
    "    plt.show()\n",
    "        \n",
    "    return model, time_batch, loss_batch, time_epoch, [loss_batch[0]] + loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HandwritingGenerationModel2(hidden_size, n_gaussians, dropout)\n",
    "model, time_batch, loss_batch, time_epoch, loss_epoch = train_network(model, epochs=10, generate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "data_loader = DataLoader(n_batch, sequence_length, 20, U_items=U_items) # 20 = datascale\n",
    "\n",
    "model = HandwritingGenerationModel2(hidden_size, n_gaussians, dropout)\n",
    "\n",
    "x, y, s, c = data_loader.next_batch()\n",
    "x = np.float32(np.array(x)) # -> (n_batch, sequence_length, 3)\n",
    "y = np.float32(np.array(y)) # -> (n_batch, sequence_length, 3)\n",
    "\n",
    "x = torch.from_numpy(x).permute(1, 0, 2) # torch.Size([sequence_length, n_batch, 3])\n",
    "y = torch.from_numpy(y).permute(1, 0, 2)\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    \n",
    "\n",
    "es, pis, mu1s, mu2s, sigma1s, sigma2s, rhos = model.forward(x)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
